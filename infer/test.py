import torch
import torch.nn.functional as F
import json
import os
import re
import sys
# 1. Setup Paths
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)
from transformers import AutoTokenizer
from model.model import ZeroShotJointModel  # ‡πÑ‡∏ü‡∏•‡πå model.py ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

class NERREPredictor:
    def __init__(self, model_dir):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Labels ‡∏à‡∏≤‡∏Å Config
        with open(os.path.join(model_dir, "config.json"), "r", encoding="utf-8") as f:
            self.config = json.load(f)
        
        self.ent_labels = self.config["ent_labels"]
        self.rel_labels = self.config["rel_labels"]
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
        self.model = ZeroShotJointModel(model_name="xlm-roberta-base")
        
        # 3. ‡πÇ‡∏´‡∏•‡∏î Weights (pytorch_model.bin)
        model_path = os.path.join(model_dir, "best_model.bin")
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
        self.model.load_state_dict(checkpoint)
        self.model.to(self.device)
        self.model.eval()
        
        # 4. ‡πÅ‡∏Ñ‡∏ä Labels ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        self._cache_labels()
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô {len(self.ent_labels)} Entities ‡πÅ‡∏•‡∏∞ {len(self.rel_labels)} Relations")

    def _cache_labels(self):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Label ‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤"""
        def tokenize(label_list):
            tokens = self.tokenizer(label_list, return_tensors="pt", padding=True, truncation=True).to(self.device)
            return tokens["input_ids"].unsqueeze(0), tokens["attention_mask"].unsqueeze(0)

        ent_ids, ent_mask = tokenize(self.ent_labels)
        rel_ids, rel_mask = tokenize(self.rel_labels)
        self.model.set_global_labels(ent_ids, ent_mask, rel_ids, rel_mask)

    def _generate_spans(self, tokens, max_width=8): # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô 8
        spans = []
        for i in range(len(tokens)):
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Token ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Special Tokens (<s>, </s>, <pad>)
            if tokens[i] in [self.tokenizer.cls_token, self.tokenizer.sep_token, self.tokenizer.pad_token]:
                continue
            for width in range(1, max_width + 1):
                if i + width <= len(tokens):
                    spans.append((i, i + width - 1))
        return spans

    def predict(self, text, conf_threshold=0.1): # ‡∏•‡∏î threshold ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        token_ids = inputs["input_ids"][0]
        tokens = self.tokenizer.convert_ids_to_tokens(token_ids)
        
        spans = self._generate_spans(tokens)
        with torch.no_grad():
            ent_logits, _ = self.model(inputs["input_ids"], inputs["attention_mask"], entity_spans=[spans])
        
        # --- [‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡πà 1: Scaling] ---
        # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà 0.12 ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏Ñ‡∏π‡∏ì Temperature ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏¢‡∏≤‡∏¢ Scale ‡∏Å‡πà‡∏≠‡∏ô Softmax
        # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ logits ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Margin
        logits = ent_logits[0] # [num_spans, num_labels]
        
        # üî• ‡∏à‡∏∏‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏ä‡πâ Temperature Scaling ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ñ‡πà‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        # ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 0.07 ‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á 0.01 ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏¢‡∏±‡∏á‡πÄ‡∏Å‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°
        tau = 0.03 
        probs = torch.softmax(logits / tau, dim=-1)
        
        max_probs, pred_ids = torch.max(probs, dim=-1)
        
        extracted_entities = []
        for i, (span, p_id, conf) in enumerate(zip(spans, pred_ids, max_probs)):
            label = self.ent_labels[p_id]
            
            # Debug: ‡∏î‡∏π‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Elon Musk
            span_text = self.tokenizer.decode(token_ids[span[0] : span[1] + 1]).strip()
            if "Elon" in span_text and label == "company":
                top2_val, top2_idx = torch.topk(probs[i], 2)
                # print(f"DEBUG: {span_text} -> Top1: {self.ent_labels[top2_idx[0]]} ({top2_val[0]:.4f}), Top2: {self.ent_labels[top2_idx[1]]} ({top2_val[1]:.4f})")

            if label != "O" and conf > conf_threshold:
                extracted_entities.append({
                    "text": span_text,
                    "type": label,
                    "conf": conf.item(),
                    "span": span
                })

        # --- ‡πÉ‡∏ä‡πâ NMS ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏à‡∏±‡∏î Span ‡∏ã‡πâ‡∏≠‡∏ô (‡πÄ‡∏ä‡πà‡∏ô He is a famous engineer) ---
        final_entities = self._apply_nms(extracted_entities)
        return final_entities

    def _apply_nms(self, entities):
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Span ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NER) ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ Conf ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î"""
        entities.sort(key=lambda x: x['conf'], reverse=True)
        final = []
        occupied = set()
        for ent in entities:
            s, e = ent['span']
            indices = set(range(s, e + 1))
            if not (indices & occupied):
                final.append(ent)
                occupied.update(indices)
        return sorted(final, key=lambda x: x['span'][0])

# --- ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ---
if __name__ == "__main__":
    MODEL_DIR = "saved_model_v16"  # ‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    predictor = NERREPredictor(MODEL_DIR)
    
    raw_text = "Elon Musk founded SpaceX in 2002. He is a famous engineer from USA."
    
    ents = predictor.predict(raw_text)
    
    print(f"\nüìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {raw_text}")
    print("\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö Entities:")
    for e in ents:
        print(f"   - {e['text']} ({e['type']}) [Conf: {e['conf']}]")
        
    # print("\nüîó ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö Relations:")
    # for r in rels:
    #     print(f"   - {r['subject']} --[{r['relation']}]--> {r['object']} (Conf: {r['conf']})")