"""
GraphRAG Dataset - Dataset class for NER + Relation Extraction
Designed for building Knowledge Graphs
"""

import json
import random
import torch
from torch.utils.data import Dataset


class GraphRAGDataset(Dataset):
    """
    Dataset for joint NER + Relation Extraction
    Suitable for Graph RAG / Knowledge Graph construction
    
    ‚úÖ Key Feature: ‡∏°‡∏µ "O" (Outside/None) label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö spans ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà entity
       ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ span ‡πÑ‡∏´‡∏ô‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô entity ‡πÅ‡∏•‡∏∞ span ‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£
    """
    
    # Special label for non-entity spans
    O_LABEL = "O"
    NO_REL_LABEL = "NO_RELATION"
    
    def __init__(self, json_file, tokenizer, max_len=256, neg_sample_ratio=0.3, neg_span_ratio=1.0):
        """
        Args:
            json_file: path to training data JSON (str or List[str])
            tokenizer: HuggingFace tokenizer
            max_len: max sequence length
            neg_sample_ratio: ratio of negative labels to sample
            neg_span_ratio: ratio of negative spans (non-entity) to positive spans
        """
        self.data = []
        if isinstance(json_file, str):
            json_files = [json_file]
        else:
            json_files = json_file
            
        for jf in json_files:
            print(f"Loading data from {jf}...")
            with open(jf, 'r', encoding='utf-8') as f:
                data_part = json.load(f)
                self.data.extend(data_part)
            
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.neg_sample_ratio = neg_sample_ratio
        self.neg_span_ratio = neg_span_ratio
        
        # Scan for all unique labels
        self.all_ent_labels = set()
        self.all_rel_labels = set()
        
        # Store descriptions
        self.ent_descriptions = {}
        self.rel_descriptions = {}

        print(f"Scanning {json_file} for labels...")
        for item in self.data:
            if 'entities' in item:
                for ent in item['entities']:
                    label = ent['label']
                    self.all_ent_labels.add(label)
                    # Capture description if available
                    if 'description' in ent and label not in self.ent_descriptions:
                        self.ent_descriptions[label] = ent['description']

            if 'relations' in item:
                for rel in item['relations']:
                    label = rel['label']
                    self.all_rel_labels.add(label)
                    # Capture description if available
                    if 'description' in rel and label not in self.rel_descriptions:
                        self.rel_descriptions[label] = rel['description']
        
        self.all_ent_labels = sorted(list(self.all_ent_labels))
        self.all_rel_labels = sorted(list(self.all_rel_labels))
        
        # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° "O" label ‡πÑ‡∏ß‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å (index 0)
        self.all_ent_labels_with_O = [self.O_LABEL] + self.all_ent_labels
        
        # üî• [NEW] ‡πÄ‡∏û‡∏¥‡πà‡∏° "NO_RELATION" label ‡πÑ‡∏ß‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å (index 0)
        self.all_rel_labels_with_NO_REL = [self.NO_REL_LABEL] + self.all_rel_labels

        self.ent_label2id = {label: i for i, label in enumerate(self.all_ent_labels_with_O)}
        self.rel_label2id = {label: i for i, label in enumerate(self.all_rel_labels_with_NO_REL)}

        # Create Descriptive Label List for Model Input
        self.ent_label_texts = []
        for label in self.all_ent_labels_with_O:
            if label == self.O_LABEL:
                desc = "Outside: Not an entity"
            else:
                desc = self.ent_descriptions.get(label, f"{label}: Representation of {label}")
                # Ensure format "Label: Description" if not already
                if not desc.startswith(label):
                    desc = f"{label}: {desc}"
            self.ent_label_texts.append(desc)

        self.rel_label_texts = []
        for label in self.all_rel_labels_with_NO_REL:
            if label == self.NO_REL_LABEL:
                desc = "No Relation: No relationship exists between these entities"
            else:
                desc = self.rel_descriptions.get(label, f"{label}: Relation type {label}")
                if not desc.startswith(label):
                    desc = f"{label}: {desc}"
            self.rel_label_texts.append(desc)
        
        print(f"‚úÖ Found {len(self.all_ent_labels)} entity types")
        print(f"‚úÖ Found {len(self.all_rel_labels)} relation types")
        print(f"‚úÖ Added special labels: '{self.O_LABEL}' and '{self.NO_REL_LABEL}'")

    def get_ent_label_list(self):
        return self.all_ent_labels_with_O

    def get_rel_label_list(self):
        return self.all_rel_labels_with_NO_REL

    def __len__(self):
        return len(self.data)

    def _char_to_token_span(self, encoding, start_char, end_char):
        """Convert character span to token span"""
        start_token = None
        end_token = None
        
        # Find start token
        for i in range(3):  # Try a few offsets
            start_token = encoding.char_to_token(start_char + i)
            if start_token is not None:
                break
                
        # Find end token (end_char is exclusive, so -1)
        for i in range(3):
            end_token = encoding.char_to_token(end_char - 1 - i)
            if end_token is not None:
                break
        
        return start_token, end_token

    def _get_word_boundaries(self, encoding, text):
        """‡∏´‡∏≤ word boundaries ‡∏à‡∏≤‡∏Å offset_mapping"""
        offset_mapping = encoding["offset_mapping"].squeeze(0).tolist()
        words = []
        current_word_tokens = []
        current_char_start = None
        prev_char_end = None
        
        for token_idx, (char_start, char_end) in enumerate(offset_mapping):
            if char_start == char_end == 0:
                continue
            if prev_char_end is not None and char_start > prev_char_end:
                if current_word_tokens:
                    words.append({
                        'token_start': current_word_tokens[0],
                        'token_end': current_word_tokens[-1],
                        'char_start': current_char_start,
                        'char_end': prev_char_end
                    })
                current_word_tokens = [token_idx]
                current_char_start = char_start
            else:
                if current_char_start is None:
                    current_char_start = char_start
                current_word_tokens.append(token_idx)
            prev_char_end = char_end
        
        if current_word_tokens:
            words.append({
                'token_start': current_word_tokens[0],
                'token_end': current_word_tokens[-1],
                'char_start': current_char_start,
                'char_end': prev_char_end
            })
        return words

    def _generate_negative_spans(self, words, valid_entities, num_to_sample, max_span_width=3):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á negative spans ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô "Hard Negatives" (‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á Entity ‡∏à‡∏£‡∏¥‡∏á)
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô
        """
        all_candidates = []
        n_words = len(words)
        
        # -------------------------------------------------------------
        # üî• ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 1: Hard Negatives (Sub-spans) - ‡∏û‡∏£‡∏∞‡πÄ‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
        # ‡∏ï‡∏±‡∏î Entity ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏´‡∏ß‡πà‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Negative
        # -------------------------------------------------------------
        valid_entity_ranges = set() # ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Entity ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏ä‡πá‡∏Ñ
        
        for ent in valid_entities:
            # ent ‡∏Ñ‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'span': (start, end)
            start, end = ent['span'] # token index
            valid_entity_ranges.add((start, end))
            
            # ‡∏ñ‡πâ‡∏≤ Entity ‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 1 token (‡πÄ‡∏ä‡πà‡∏ô "Elon Musk")
            # ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á sub-span (‡πÄ‡∏ä‡πà‡∏ô "Elon", "Musk") ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Negative
            span_len = end - start + 1
            if span_len > 1:
                # Loop ‡∏™‡∏£‡πâ‡∏≤‡∏á sub-spans ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏°‡∏±‡∏ô‡πÄ‡∏≠‡∏á
                for i in range(span_len):
                    for j in range(i, span_len):
                        sub_start = start + i
                        sub_end = start + j
                        
                        # ‡∏ñ‡πâ‡∏≤ sub-span ‡∏ô‡∏µ‡πâ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏° (‡∏Ñ‡∏∑‡∏≠‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°)
                        if not (sub_start == start and sub_end == end):
                            all_candidates.append((sub_start, sub_end))

        # -------------------------------------------------------------
        # ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 2: Random Negatives (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        # ‡∏™‡∏∏‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        # -------------------------------------------------------------
        for width in range(1, min(max_span_width + 1, n_words + 1)):
            for start_idx in range(n_words - width + 1):
                end_idx = start_idx + width - 1
                
                # ‡∏ñ‡πâ‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Entity ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å set ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ)
                if (start_idx, end_idx) not in valid_entity_ranges:
                     # (Optional) ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö Hard Negatives ‡∏ó‡∏µ‡πà‡πÉ‡∏™‡πà‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡πÑ‡∏î‡πâ 
                     # ‡πÅ‡∏ï‡πà‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏¢‡πâ‡∏≥
                    all_candidates.append((start_idx, end_idx))
        
        # ‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        if all_candidates and num_to_sample > 0:
            # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ Hard Negatives ‡πÄ‡∏¢‡∏≠‡∏∞‡∏´‡∏ô‡πà‡∏≠‡∏¢ 
            # (‡πÅ‡∏ï‡πà‡πÉ‡∏ô list all_candidates ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏õ‡∏ô‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ)
            negative_spans = random.sample(
                all_candidates, 
                k=min(len(all_candidates), num_to_sample)
            )
            return negative_spans
            
        return []
    

    def _generate_border_negatives(self, valid_entities, num_to_sample, seq_len):
        """
        ‡∏à‡∏á‡πÉ‡∏à‡∏™‡∏£‡πâ‡∏≤‡∏á Span ‡∏ó‡∏µ‡πà '‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏°‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï (Boundary Control)
        """
        border_candidates = []
        for ent in valid_entities:
            s, e = ent['span']
            
            # ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏≠‡∏Å 1: ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤‡πÑ‡∏õ‡∏≠‡∏µ‡∏Å 1 token (‡πÄ‡∏ä‡πà‡∏ô "Elon Musk" -> "Elon Musk founded")
            if e + 1 < seq_len:
                border_candidates.append((s, e + 1))
            
            # ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏≠‡∏Å 2: ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ã‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏≠‡∏µ‡∏Å 1 token (‡πÄ‡∏ä‡πà‡∏ô "SpaceX" -> "founded SpaceX")
            if s - 1 >= 0:
                border_candidates.append((max(0, s - 1), e))
                
            # ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏≠‡∏Å 3: ‡∏ï‡∏±‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å 1 token (‡πÄ‡∏ä‡πà‡∏ô "Elon Musk" -> "Elon")
            if e > s:
                border_candidates.append((s, e - 1))

        if not border_candidates:
            return []
            
        return random.sample(border_candidates, k=min(len(border_candidates), num_to_sample))

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        entities = item.get('entities', [])
        relations = item.get('relations', [])
        
        # 1. Tokenize
        encoding = self.tokenizer(
            text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_offsets_mapping=True
        )
        
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        
        # 2. Convert entity spans from char to token indices
        valid_entities = []  # (token_start, token_end, label, original_idx)
        valid_entity_char_ranges = []  # For negative span sampling
        
        for ent_idx, ent in enumerate(entities):
            start_token, end_token = self._char_to_token_span(
                encoding, ent['start'], ent['end']
            )
            
            if start_token is not None and end_token is not None:
                valid_entities.append({
                    'span': (start_token, end_token),
                    'label': ent['label'],
                    'original_idx': ent_idx
                })
                valid_entity_char_ranges.append((ent['start'], ent['end']))
        
        # ‚úÖ 3. Generate negative spans (non-entity spans labeled as "O")
        words = self._get_word_boundaries(encoding, text)
        num_neg_spans = max(1, int(len(valid_entities) * self.neg_span_ratio))
        negative_spans = self._generate_negative_spans(
            words, valid_entities, num_neg_spans
        )


        # üî• [NEW] ‡πÄ‡∏û‡∏¥‡πà‡∏° Border Negatives ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        border_negatives = self._generate_border_negatives(valid_entities, len(valid_entities), self.max_len)
        
        # 4. Combine positive and negative spans
        # Positive spans (real entities)
        all_spans = []
        all_span_labels = []  # The actual label for each span
        
        for ent in valid_entities:
            all_spans.append(ent['span'])
            all_span_labels.append(ent['label'])  # e.g., "person", "organisation"
        
        # Negative spans (non-entities)
        for neg_span in negative_spans:
            all_spans.append(neg_span)
            all_span_labels.append(self.O_LABEL)  # "O" for Outside


        # üî• [NEW] ‡πÉ‡∏™‡πà Border Negatives ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        valid_span_set = set([ent['span'] for ent in valid_entities])
        for b_neg in border_negatives:
            if b_neg not in valid_span_set: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á
                all_spans.append(b_neg)
                all_span_labels.append(self.O_LABEL)
        
        # ‚úÖ 5. Use ALL labels including "O" - always use the full label set
        train_ent_labels = self.ent_label_texts  # Use descriptions
        
        # 6. Create entity target matrix
        num_spans = len(all_spans)
        num_ent_labels = len(train_ent_labels)
        
        ent_targets = torch.zeros((num_spans, num_ent_labels))

        # üî• [FIX] ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ 1.0 ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Label ‡∏à‡∏£‡∏¥‡∏á
        for i, label_text in enumerate(all_span_labels):
            # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
            if label_text in self.ent_label2id:
                label_idx = self.ent_label2id[label_text]
                ent_targets[i, label_idx] = 1.0
            else:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏¢‡∏ô‡∏•‡∏á "O"
                if self.O_LABEL in self.ent_label2id:
                    o_idx = self.ent_label2id[self.O_LABEL]
                    ent_targets[i, o_idx] = 1.0
        
        # ===========================================================
        # üî• [UPDATED FIX] ‡πÉ‡∏ä‡πâ Hybrid Mapping (ID ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å, Text ‡∏™‡∏≥‡∏£‡∏≠‡∏á)
        # ===========================================================
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Maps ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ 2 ‡πÅ‡∏ö‡∏ö
        id_to_valid_indices = {}    # ‚úÖ ‡πÅ‡∏ö‡∏ö‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡πÉ‡∏ä‡πâ original_idx ‡∏à‡∏≤‡∏Å JSON)
        text_to_valid_indices = {}  # ‚ö†Ô∏è ‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á (‡πÉ‡∏ä‡πâ text)

        for new_idx, ent in enumerate(valid_entities):
            # --- A. Map by ID (Original Index) ---
            orig_idx = ent['original_idx']
            if orig_idx not in id_to_valid_indices:
                id_to_valid_indices[orig_idx] = []
            id_to_valid_indices[orig_idx].append(new_idx)
            
            # --- B. Map by Text (Fallback) ---
            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å JSON ‡πÄ‡∏î‡∏¥‡∏°
            orig_ent_data = entities[orig_idx] 
            # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô JSON ‡∏°‡∏µ key 'text' ‡∏Å‡πá‡πÉ‡∏ä‡πâ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡∏ï‡∏±‡∏î string ‡πÄ‡∏≠‡∏≤
            entity_text = orig_ent_data.get('text', text[orig_ent_data['start']:orig_ent_data['end']])
            
            if entity_text not in text_to_valid_indices:
                text_to_valid_indices[entity_text] = []
            text_to_valid_indices[entity_text].append(new_idx)
        
        # 2. ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Relation Mapping)
        positive_rel_map = {}
        
        for rel in relations:
            head_indices = []
            tail_indices = []

            # üî• Priority 1: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ JSON ‡∏°‡∏µ 'head_idx' / 'tail_idx' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö CrossRE ver ‡πÄ‡∏Å‡πà‡∏≤)
            if 'head_idx' in rel and 'tail_idx' in rel:
                head_indices = id_to_valid_indices.get(rel['head_idx'], [])
                tail_indices = id_to_valid_indices.get(rel['tail_idx'], [])

            # üî• Priority 2: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ 'head' / 'tail' ‡πÄ‡∏õ‡πá‡∏ô Integer Index ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö hf_dataloader format)
            elif 'head' in rel and 'tail' in rel and isinstance(rel['head'], int) and isinstance(rel['tail'], int):
                head_indices = id_to_valid_indices.get(rel['head'], [])
                tail_indices = id_to_valid_indices.get(rel['tail'], [])

            # ‚ö†Ô∏è Priority 3: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ (Text) ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Data v2 ‡πÄ‡∏î‡∏¥‡∏°)
            elif 'head' in rel and 'tail' in rel:
                head_indices = text_to_valid_indices.get(rel['head'], [])
                tail_indices = text_to_valid_indices.get(rel['tail'], [])
            
            # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ Entity ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏ï‡∏≠‡∏ô Tokenize) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            if not head_indices or not tail_indices:
                continue

            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Target (One-hot vector)
            # ‡πÉ‡∏ä‡πâ self.all_rel_labels_with_NO_REL (‡∏ó‡∏µ‡πà‡∏°‡∏µ NO_RELATION ‡∏ó‡∏µ‡πà index 0)
            rel_target = torch.zeros(len(self.all_rel_labels_with_NO_REL))
            
            if rel['label'] in self.rel_label2id:
                rel_idx = self.rel_label2id[rel['label']]
                rel_target[rel_idx] = 1.0
            
            # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (Pairing)
            for h_idx in head_indices:
                for t_idx in tail_indices:
                    if h_idx == t_idx: continue # ‡∏Ç‡πâ‡∏≤‡∏° Self-loop
                    
                    pair_key = (h_idx, t_idx)
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° Logic (OR) ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö
                    if pair_key in positive_rel_map:
                        positive_rel_map[pair_key] = torch.max(positive_rel_map[pair_key], rel_target)
                    else:
                        positive_rel_map[pair_key] = rel_target

        # ... (‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏™‡πà‡∏ß‡∏ô Negative Sampling ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢) ...

        # -----------------------------------------------------------
        # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏î‡πà‡∏ß‡∏ô: ‡πÉ‡∏ä‡πâ "Negative Sampling" ‡πÅ‡∏ó‡∏ô "All Negatives"
        # -----------------------------------------------------------
        all_pairs = []
        all_targets = []
        
        # 1. ‡πÉ‡∏™‡πà Positive Pairs (‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Reproducible (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
        pos_keys = sorted(list(positive_rel_map.keys()))
        for pair in pos_keys:
            all_pairs.append(pair)
            all_targets.append(positive_rel_map[pair])
            
        num_positives = len(pos_keys)
        
        # 2. ‡πÄ‡∏Å‡πá‡∏ö Negative Candidates (‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå)
        neg_candidates = []
        num_entities = len(valid_entities)
        
        if num_entities > 1:
            for i in range(num_entities):
                for j in range(num_entities):
                    if i == j: continue 
                    pair = (i, j)
                    if pair not in positive_rel_map:
                        neg_candidates.append(pair)
        
        # 3. üî• Hard Negative Strategy: Reversals + Random
        # ‡∏Å‡∏é: ‡πÄ‡∏≠‡∏≤ Negative ‡πÅ‡∏Ñ‡πà 3 ‡πÄ‡∏ó‡πà‡∏≤‡∏Ç‡∏≠‡∏á Positive (Ratio 1:3)
        if num_positives > 0:
            num_neg_to_sample = num_positives * 9
        else:
            num_neg_to_sample = 15

        # 3.1 Force Reversed Pairs (‡∏™‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ A->B ‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö B->A)
        forced_negatives = set()
        for (h, t) in pos_keys:
             rev_pair = (t, h)
             if rev_pair not in positive_rel_map:
                 forced_negatives.add(rev_pair)

        # 3.2 Fill the rest with Random Negatives
        neg_candidates = [p for p in neg_candidates if p not in forced_negatives] # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥
        
        final_negs = list(forced_negatives)
        
        # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°
        remaining_slots = num_neg_to_sample - len(final_negs)
        if remaining_slots > 0 and neg_candidates:
             final_negs += random.sample(neg_candidates, min(len(neg_candidates), remaining_slots))
        
        # ‡∏ñ‡πâ‡∏≤ Hard Negatives ‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤ ‡∏Å‡πá‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ (‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏¢‡∏≠‡∏∞‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Direction)
        
        if final_negs:
             # üî• [NET] Negative Target ‡∏Ñ‡∏∑‡∏≠ class "NO_RELATION" (index 0)
            zero_target = torch.zeros(len(self.all_rel_labels_with_NO_REL))
            zero_target[0] = 1.0 # Set NO_RELATION to 1.0
            
            for pair in final_negs:
                all_pairs.append(pair)
                all_targets.append(zero_target)

        # 4. Stack Targets (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        if all_targets:
            rel_targets = torch.stack(all_targets)
        else:
            rel_targets = torch.zeros((0, len(self.all_rel_labels_with_NO_REL)))
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "spans": all_spans,
            "ent_labels": train_ent_labels,
            "ent_targets": ent_targets,
            "rel_pairs": all_pairs,      # ‚úÖ ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏π‡πà‡∏´‡∏•‡∏≠‡∏Å
            "rel_labels": self.rel_label_texts, # Use descriptions
            "rel_targets": rel_targets,  # ‚úÖ Target ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á 1 ‡πÅ‡∏•‡∏∞ 0
            "num_positive_spans": len(valid_entities)
        }


def graph_rag_collate_fn(batch):
    """
    Collate function for variable-length data
    """
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    
    # Keep variable-length items as lists
    spans = [item['spans'] for item in batch]
    ent_labels = [item['ent_labels'] for item in batch]
    rel_labels = [item['rel_labels'] for item in batch]
    ent_targets = [item['ent_targets'] for item in batch]
    rel_targets = [item['rel_targets'] for item in batch]
    rel_pairs = [item['rel_pairs'] for item in batch]
    num_positive = [item['num_positive_spans'] for item in batch]

    return {
        "text_ids": input_ids,
        "text_mask": attention_mask,
        "spans": spans,
        "ent_labels_text": ent_labels,
        "rel_labels_text": rel_labels,
        "ent_targets": ent_targets,
        "rel_targets": rel_targets,
        "pairs": rel_pairs,
        "num_positive_spans": num_positive  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö filter relation pairs
    }


# Backward compatibility
ZeroShotDataset = GraphRAGDataset
collate_fn = graph_rag_collate_fn
