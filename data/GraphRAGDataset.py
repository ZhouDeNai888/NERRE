"""
GraphRAG Dataset - Dataset class for NER + Relation Extraction
Designed for building Knowledge Graphs
"""

import json
import random
import torch
from torch.utils.data import Dataset


class GraphRAGDataset(Dataset):
    """
    Dataset for joint NER + Relation Extraction
    Suitable for Graph RAG / Knowledge Graph construction
    
    ‚úÖ Key Feature: ‡∏°‡∏µ "O" (Outside/None) label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö spans ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà entity
       ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ span ‡πÑ‡∏´‡∏ô‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô entity ‡πÅ‡∏•‡∏∞ span ‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£
    """
    
    # Special label for non-entity spans
    O_LABEL = "O"
    
    def __init__(self, json_file, tokenizer, max_len=256, neg_sample_ratio=0.3, neg_span_ratio=1.0):
        """
        Args:
            json_file: path to training data JSON
            tokenizer: HuggingFace tokenizer
            max_len: max sequence length
            neg_sample_ratio: ratio of negative labels to sample
            neg_span_ratio: ratio of negative spans (non-entity) to positive spans
        """
        with open(json_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
            
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.neg_sample_ratio = neg_sample_ratio
        self.neg_span_ratio = neg_span_ratio
        
        # Scan for all unique labels
        self.all_ent_labels = set()
        self.all_rel_labels = set()
        
        print(f"Scanning {json_file} for labels...")
        for item in self.data:
            if 'entities' in item:
                for ent in item['entities']:
                    self.all_ent_labels.add(ent['label'])
            if 'relations' in item:
                for rel in item['relations']:
                    self.all_rel_labels.add(rel['label'])
        
        self.all_ent_labels = sorted(list(self.all_ent_labels))
        self.all_rel_labels = sorted(list(self.all_rel_labels))
        
        # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° "O" label ‡πÑ‡∏ß‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å (index 0)
        self.all_ent_labels_with_O = [self.O_LABEL] + self.all_ent_labels

        self.ent_label2id = {label: i for i, label in enumerate(self.all_ent_labels_with_O)}
        
        print(f"‚úÖ Found {len(self.all_ent_labels)} entity types: {self.all_ent_labels}")
        print(f"‚úÖ Added 'O' label for non-entity spans")
        print(f"‚úÖ Found {len(self.all_rel_labels)} relation types: {self.all_rel_labels}")

    def __len__(self):
        return len(self.data)

    def _char_to_token_span(self, encoding, start_char, end_char):
        """Convert character span to token span"""
        start_token = None
        end_token = None
        
        # Find start token
        for i in range(3):  # Try a few offsets
            start_token = encoding.char_to_token(start_char + i)
            if start_token is not None:
                break
                
        # Find end token (end_char is exclusive, so -1)
        for i in range(3):
            end_token = encoding.char_to_token(end_char - 1 - i)
            if end_token is not None:
                break
        
        return start_token, end_token

    def _get_word_boundaries(self, encoding, text):
        """‡∏´‡∏≤ word boundaries ‡∏à‡∏≤‡∏Å offset_mapping"""
        offset_mapping = encoding["offset_mapping"].squeeze(0).tolist()
        words = []
        current_word_tokens = []
        current_char_start = None
        prev_char_end = None
        
        for token_idx, (char_start, char_end) in enumerate(offset_mapping):
            if char_start == char_end == 0:
                continue
            if prev_char_end is not None and char_start > prev_char_end:
                if current_word_tokens:
                    words.append({
                        'token_start': current_word_tokens[0],
                        'token_end': current_word_tokens[-1],
                        'char_start': current_char_start,
                        'char_end': prev_char_end
                    })
                current_word_tokens = [token_idx]
                current_char_start = char_start
            else:
                if current_char_start is None:
                    current_char_start = char_start
                current_word_tokens.append(token_idx)
            prev_char_end = char_end
        
        if current_word_tokens:
            words.append({
                'token_start': current_word_tokens[0],
                'token_end': current_word_tokens[-1],
                'char_start': current_char_start,
                'char_end': prev_char_end
            })
        return words

    def _generate_negative_spans(self, words, valid_entities, num_to_sample, max_span_width=3):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á negative spans ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô "Hard Negatives" (‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á Entity ‡∏à‡∏£‡∏¥‡∏á)
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô
        """
        all_candidates = []
        n_words = len(words)
        
        # -------------------------------------------------------------
        # üî• ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 1: Hard Negatives (Sub-spans) - ‡∏û‡∏£‡∏∞‡πÄ‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
        # ‡∏ï‡∏±‡∏î Entity ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏´‡∏ß‡πà‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Negative
        # -------------------------------------------------------------
        valid_entity_ranges = set() # ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Entity ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏ä‡πá‡∏Ñ
        
        for ent in valid_entities:
            # ent ‡∏Ñ‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'span': (start, end)
            start, end = ent['span'] # token index
            valid_entity_ranges.add((start, end))
            
            # ‡∏ñ‡πâ‡∏≤ Entity ‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 1 token (‡πÄ‡∏ä‡πà‡∏ô "Elon Musk")
            # ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á sub-span (‡πÄ‡∏ä‡πà‡∏ô "Elon", "Musk") ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Negative
            span_len = end - start + 1
            if span_len > 1:
                # Loop ‡∏™‡∏£‡πâ‡∏≤‡∏á sub-spans ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏°‡∏±‡∏ô‡πÄ‡∏≠‡∏á
                for i in range(span_len):
                    for j in range(i, span_len):
                        sub_start = start + i
                        sub_end = start + j
                        
                        # ‡∏ñ‡πâ‡∏≤ sub-span ‡∏ô‡∏µ‡πâ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏° (‡∏Ñ‡∏∑‡∏≠‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°)
                        if not (sub_start == start and sub_end == end):
                            all_candidates.append((sub_start, sub_end))

        # -------------------------------------------------------------
        # ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 2: Random Negatives (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        # ‡∏™‡∏∏‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        # -------------------------------------------------------------
        for width in range(1, min(max_span_width + 1, n_words + 1)):
            for start_idx in range(n_words - width + 1):
                end_idx = start_idx + width - 1
                
                # ‡∏ñ‡πâ‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Entity ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å set ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ)
                if (start_idx, end_idx) not in valid_entity_ranges:
                     # (Optional) ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö Hard Negatives ‡∏ó‡∏µ‡πà‡πÉ‡∏™‡πà‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡πÑ‡∏î‡πâ 
                     # ‡πÅ‡∏ï‡πà‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏¢‡πâ‡∏≥
                    all_candidates.append((start_idx, end_idx))
        
        # ‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        if all_candidates and num_to_sample > 0:
            # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ Hard Negatives ‡πÄ‡∏¢‡∏≠‡∏∞‡∏´‡∏ô‡πà‡∏≠‡∏¢ 
            # (‡πÅ‡∏ï‡πà‡πÉ‡∏ô list all_candidates ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏õ‡∏ô‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ)
            negative_spans = random.sample(
                all_candidates, 
                k=min(len(all_candidates), num_to_sample)
            )
            return negative_spans
            
        return []
        

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        entities = item.get('entities', [])
        relations = item.get('relations', [])
        
        # 1. Tokenize
        encoding = self.tokenizer(
            text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_offsets_mapping=True
        )
        
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        
        # 2. Convert entity spans from char to token indices
        valid_entities = []  # (token_start, token_end, label, original_idx)
        valid_entity_char_ranges = []  # For negative span sampling
        
        for ent_idx, ent in enumerate(entities):
            start_token, end_token = self._char_to_token_span(
                encoding, ent['start'], ent['end']
            )
            
            if start_token is not None and end_token is not None:
                valid_entities.append({
                    'span': (start_token, end_token),
                    'label': ent['label'],
                    'original_idx': ent_idx
                })
                valid_entity_char_ranges.append((ent['start'], ent['end']))
        
        # ‚úÖ 3. Generate negative spans (non-entity spans labeled as "O")
        words = self._get_word_boundaries(encoding, text)
        num_neg_spans = max(1, int(len(valid_entities) * self.neg_span_ratio))
        negative_spans = self._generate_negative_spans(
            words, valid_entities, num_neg_spans
        )
        
        # 4. Combine positive and negative spans
        # Positive spans (real entities)
        all_spans = []
        all_span_labels = []  # The actual label for each span
        
        for ent in valid_entities:
            all_spans.append(ent['span'])
            all_span_labels.append(ent['label'])  # e.g., "person", "organisation"
        
        # Negative spans (non-entities)
        for neg_span in negative_spans:
            all_spans.append(neg_span)
            all_span_labels.append(self.O_LABEL)  # "O" for Outside
        
        # ‚úÖ 5. Use ALL labels including "O" - always use the full label set
        train_ent_labels = self.all_ent_labels_with_O  # ["O", "algorithm", "date", ...]
        
        # 6. Create entity target matrix
        num_spans = len(all_spans)
        num_ent_labels = len(train_ent_labels)
        
        ent_targets = torch.zeros((num_spans, num_ent_labels))

        # üî• [FIX] ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ 1.0 ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Label ‡∏à‡∏£‡∏¥‡∏á
        for i, label_text in enumerate(all_span_labels):
            # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
            if label_text in self.ent_label2id:
                label_idx = self.ent_label2id[label_text]
                ent_targets[i, label_idx] = 1.0
            else:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏¢‡∏ô‡∏•‡∏á "O"
                if self.O_LABEL in self.ent_label2id:
                    o_idx = self.ent_label2id[self.O_LABEL]
                    ent_targets[i, o_idx] = 1.0
        
        # ===========================================================
        # üî• [UPDATED FIX] ‡πÉ‡∏ä‡πâ Hybrid Mapping (ID ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å, Text ‡∏™‡∏≥‡∏£‡∏≠‡∏á)
        # ===========================================================
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Maps ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ 2 ‡πÅ‡∏ö‡∏ö
        id_to_valid_indices = {}    # ‚úÖ ‡πÅ‡∏ö‡∏ö‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡πÉ‡∏ä‡πâ original_idx ‡∏à‡∏≤‡∏Å JSON)
        text_to_valid_indices = {}  # ‚ö†Ô∏è ‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á (‡πÉ‡∏ä‡πâ text)

        for new_idx, ent in enumerate(valid_entities):
            # --- A. Map by ID (Original Index) ---
            orig_idx = ent['original_idx']
            if orig_idx not in id_to_valid_indices:
                id_to_valid_indices[orig_idx] = []
            id_to_valid_indices[orig_idx].append(new_idx)
            
            # --- B. Map by Text (Fallback) ---
            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å JSON ‡πÄ‡∏î‡∏¥‡∏°
            orig_ent_data = entities[orig_idx] 
            # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô JSON ‡∏°‡∏µ key 'text' ‡∏Å‡πá‡πÉ‡∏ä‡πâ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡∏ï‡∏±‡∏î string ‡πÄ‡∏≠‡∏≤
            entity_text = orig_ent_data.get('text', text[orig_ent_data['start']:orig_ent_data['end']])
            
            if entity_text not in text_to_valid_indices:
                text_to_valid_indices[entity_text] = []
            text_to_valid_indices[entity_text].append(new_idx)
        
        # 2. ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Relation Mapping)
        positive_rel_map = {}
        
        for rel in relations:
            head_indices = []
            tail_indices = []

            # üî• Priority 1: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ JSON ‡∏°‡∏µ 'head_idx' / 'tail_idx' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
            if 'head_idx' in rel and 'tail_idx' in rel:
                head_indices = id_to_valid_indices.get(rel['head_idx'], [])
                tail_indices = id_to_valid_indices.get(rel['tail_idx'], [])

            # ‚ö†Ô∏è Priority 2: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ (Text) ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
            elif 'head' in rel and 'tail' in rel:
                head_indices = text_to_valid_indices.get(rel['head'], [])
                tail_indices = text_to_valid_indices.get(rel['tail'], [])
            
            # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ Entity ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏ï‡∏≠‡∏ô Tokenize) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            if not head_indices or not tail_indices:
                continue

            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Target (One-hot vector)
            rel_target = torch.zeros(len(self.all_rel_labels))
            if rel['label'] in self.all_rel_labels:
                rel_idx = self.all_rel_labels.index(rel['label']) # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ self.label2id ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß
                rel_target[rel_idx] = 1.0
            
            # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (Pairing)
            for h_idx in head_indices:
                for t_idx in tail_indices:
                    if h_idx == t_idx: continue # ‡∏Ç‡πâ‡∏≤‡∏° Self-loop
                    
                    pair_key = (h_idx, t_idx)
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° Logic (OR) ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö
                    if pair_key in positive_rel_map:
                        positive_rel_map[pair_key] = torch.max(positive_rel_map[pair_key], rel_target)
                    else:
                        positive_rel_map[pair_key] = rel_target

        # ... (‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏™‡πà‡∏ß‡∏ô Negative Sampling ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢) ...

        # -----------------------------------------------------------
        # üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏î‡πà‡∏ß‡∏ô: ‡πÉ‡∏ä‡πâ "Negative Sampling" ‡πÅ‡∏ó‡∏ô "All Negatives"
        # -----------------------------------------------------------
        all_pairs = []
        all_targets = []
        
        # 1. ‡πÉ‡∏™‡πà Positive Pairs (‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Reproducible (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
        pos_keys = sorted(list(positive_rel_map.keys()))
        for pair in pos_keys:
            all_pairs.append(pair)
            all_targets.append(positive_rel_map[pair])
            
        num_positives = len(pos_keys)
        
        # 2. ‡πÄ‡∏Å‡πá‡∏ö Negative Candidates (‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå)
        neg_candidates = []
        num_entities = len(valid_entities)
        
        if num_entities > 1:
            for i in range(num_entities):
                for j in range(num_entities):
                    if i == j: continue 
                    pair = (i, j)
                    if pair not in positive_rel_map:
                        neg_candidates.append(pair)
        
        # 3. üî• ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Negative ‡∏°‡∏≤‡πÅ‡∏Ñ‡πà‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô (Sampling)
        # ‡∏Å‡∏é: ‡πÄ‡∏≠‡∏≤ Negative ‡πÅ‡∏Ñ‡πà 3 ‡πÄ‡∏ó‡πà‡∏≤‡∏Ç‡∏≠‡∏á Positive ‡∏Å‡πá‡∏û‡∏≠ (Ratio 1:3)
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Positive ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡∏™‡∏±‡∏Å 2-3 ‡∏ï‡∏±‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≠‡∏ô‡∏ß‡πà‡∏≤ "‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡∏∞"
        
        if num_positives > 0:
            num_neg_to_sample = min(len(neg_candidates), num_positives * 3) # ‚úÖ Ratio 1:3
        else:
            num_neg_to_sample = min(len(neg_candidates), 5) # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Positive ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡∏™‡∏≠‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢
            
        if neg_candidates:
            # ‡πÉ‡∏ä‡πâ random.sample ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
            selected_negs = random.sample(neg_candidates, num_neg_to_sample)
            
            zero_target = torch.zeros(len(self.all_rel_labels))
            for pair in selected_negs:
                all_pairs.append(pair)
                all_targets.append(zero_target)

        # 4. Stack Targets (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        if all_targets:
            rel_targets = torch.stack(all_targets)
        else:
            rel_targets = torch.zeros((0, len(self.all_rel_labels)))
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "spans": all_spans,
            "ent_labels": train_ent_labels,
            "ent_targets": ent_targets,
            "rel_pairs": all_pairs,      # ‚úÖ ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏π‡πà‡∏´‡∏•‡∏≠‡∏Å
            "rel_labels": self.all_rel_labels,
            "rel_targets": rel_targets,  # ‚úÖ Target ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á 1 ‡πÅ‡∏•‡∏∞ 0
            "num_positive_spans": len(valid_entities)
        }


def graph_rag_collate_fn(batch):
    """
    Collate function for variable-length data
    """
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    
    # Keep variable-length items as lists
    spans = [item['spans'] for item in batch]
    ent_labels = [item['ent_labels'] for item in batch]
    rel_labels = [item['rel_labels'] for item in batch]
    ent_targets = [item['ent_targets'] for item in batch]
    rel_targets = [item['rel_targets'] for item in batch]
    rel_pairs = [item['rel_pairs'] for item in batch]
    num_positive = [item['num_positive_spans'] for item in batch]

    return {
        "text_ids": input_ids,
        "text_mask": attention_mask,
        "spans": spans,
        "ent_labels_text": ent_labels,
        "rel_labels_text": rel_labels,
        "ent_targets": ent_targets,
        "rel_targets": rel_targets,
        "pairs": rel_pairs,
        "num_positive_spans": num_positive  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö filter relation pairs
    }


# Backward compatibility
ZeroShotDataset = GraphRAGDataset
collate_fn = graph_rag_collate_fn
