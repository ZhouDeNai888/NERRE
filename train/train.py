import os
import sys
import json
# 1. ‡∏´‡∏≤ path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏°‡πà (NERRE)
# (__file__ ‡∏Ñ‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô -> dirname ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1 ‡πÑ‡∏î‡πâ 'train/' -> dirname ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2 ‡πÑ‡∏î‡πâ 'NERRE/')
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
sys.path.append(parent_dir)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast 
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm

# --- Imports ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ---
from loss_fn.focal_loss import SigmoidFocalLoss
from loss_fn.AsymmetricFocalLoss import AsymmetricFocalLoss
from model.model import ZeroShotJointModel 
import train_config as config
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import Subset
from torch.utils.data import random_split
from torch.utils.data import WeightedRandomSampler
# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Dataset - ‡πÉ‡∏ä‡πâ GraphRAGDataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Graph RAG
from data.GraphRAGDataset import GraphRAGDataset, graph_rag_collate_fn

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå training data ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if not os.path.exists(config.TRAIN_FILE):
    print(f"‚ùå Train file not found: {config.TRAIN_FILE}")
    print("   Please create a training dataset first.")
    sys.exit(1)
else:
    print(f"‚úÖ Found training file: {config.TRAIN_FILE}")



# ==========================================
# Helper Function: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Label ‡∏ó‡∏µ‡πà‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
# ==========================================
def prepare_batch_inputs(batch, tokenizer, device):
    """
    ‡πÅ‡∏õ‡∏•‡∏á List of Lists of Strings (Labels) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Tensor ‡∏û‡∏£‡πâ‡∏≠‡∏° Padding
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Model ‡πÑ‡∏î‡πâ
    """
    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Entity Labels
    # ent_labels_text ‡πÄ‡∏õ‡πá‡∏ô List[List[str]] ‡πÄ‡∏ä‡πà‡∏ô [['Per', 'Org'], ['Loc']]
    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Tensor ‡∏Ç‡∏ô‡∏≤‡∏î [Batch, Max_Num_Labels, Seq_Len]
    
    batch_ent_labels = batch['ent_labels_text']
    
    # ‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Label ‡∏ó‡∏µ‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô Batch ‡∏ô‡∏µ‡πâ
    max_ent_labels = max(len(labels) for labels in batch_ent_labels)
    
    # Flatten ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Tokenize ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏£‡πá‡∏ß‡∏ß‡∏Å‡∏ß‡πà‡∏≤ loop)
    # ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ sample ‡∏°‡∏µ‡∏Å‡∏µ‡πà label
    flat_ent_labels = []
    for labels in batch_ent_labels:
        flat_ent_labels.extend(labels)
        # ‡πÄ‡∏ï‡∏¥‡∏° Dummy label ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö Max (Padding logic)
        flat_ent_labels.extend(["O"] * (max_ent_labels - len(labels)))

    # Tokenize
    ent_inputs = tokenizer(flat_ent_labels, return_tensors="pt", padding=True, truncation=True).to(device)
    
    # Reshape ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô [Batch, Max_Labels, Seq_Len]
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Model ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Label ‡∏Ç‡∏≠‡∏á Sample ‡πÑ‡∏´‡∏ô
    b = len(batch_ent_labels)
    seq_len = ent_inputs['input_ids'].shape[1]
    
    ent_label_ids = ent_inputs['input_ids'].view(b, max_ent_labels, seq_len)
    ent_label_mask = ent_inputs['attention_mask'].view(b, max_ent_labels, seq_len)
    
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Entity Targets (Padding)
    # ent_targets ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô List of Tensors [Spans, Num_Labels]
    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á Pad ‡πÉ‡∏´‡πâ Num_Labels ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö max_ent_labels
    padded_ent_targets = []
    for i, t in enumerate(batch['ent_targets']):
        # t shape: [Num_Spans, Num_Actual_Labels]
        # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£: [Num_Spans, Max_Ent_Labels]
        num_spans, num_actual = t.shape
        pad_size = max_ent_labels - num_actual
        
        if pad_size > 0:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡πà‡∏ô Zero ‡∏°‡∏≤‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
            padding = torch.zeros((num_spans, pad_size))
            t_padded = torch.cat([t, padding], dim=1)
        else:
            t_padded = t
        padded_ent_targets.append(t_padded.to(device))

    # --- ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö Relation (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ---
    # (‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡∏•‡∏∞‡πÑ‡∏ß‡πâ ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Entity)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ Relation Labels ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á Batch ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢
    rel_inputs = tokenizer(batch['rel_labels_text'][0], return_tensors="pt", padding=True, truncation=True).to(device)
    rel_label_ids = rel_inputs['input_ids'].unsqueeze(0).repeat(b, 1, 1) # Repeat ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤ Batch
    rel_label_mask = rel_inputs['attention_mask'].unsqueeze(0).repeat(b, 1, 1)
    
    # Pad Relation Targets (‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ Entity)
    padded_rel_targets = [t.to(device) for t in batch['rel_targets']]

    return (ent_label_ids, ent_label_mask), padded_ent_targets, \
           (rel_label_ids, rel_label_mask), padded_rel_targets


# Inside your GraphRAGDataset or as a wrapper
def get_curriculum_indices(dataset, stage):
    indices = []
    for i in range(len(dataset)):
        sample = dataset.data[i] # Assuming raw data is accessible
        text_len = len(sample['text'].split())
        num_ents = len(sample['entities'])
        
        # Define Stage logic
        if stage == 1: # Easy
            if text_len < 15 and num_ents <= 2:
                indices.append(i)
        elif stage == 2: # Medium
            if text_len < 30 and num_ents <= 5:
                indices.append(i)
        else: # Stage 3: All data (Hard)
            indices.append(i)
    return indices


def get_sample_weights(model, dataloader, device, tokenizer):
    model.eval()
    all_losses = []
    
    # ent_criterion = SigmoidFocalLoss(alpha=config.ALPHA, gamma=config.GAMMA, reduction='mean')
    ent_criterion = AsymmetricFocalLoss(alpha=config.ALPHA, gamma_pos=config.POS_GAMMA, gamma_neg=config.NEG_GAMMA, reduction='mean')
    
    print("üîç Mining Hard Negatives (Calculating Sample Weights)...")
    with torch.no_grad():
        for batch in tqdm(dataloader):
            text_ids = batch['text_ids'].to(device)
            text_mask = batch['text_mask'].to(device)
            
            (ent_lbl_ids, ent_lbl_mask), ent_targets, \
            (rel_lbl_ids, rel_lbl_mask), rel_targets = prepare_batch_inputs(batch, tokenizer, device)

            # ‡∏£‡∏±‡∏ô Forward ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Logits
            ent_logits, _ = model(
                text_ids, text_mask, ent_lbl_ids, ent_lbl_mask,
                rel_lbl_ids, rel_lbl_mask, batch['spans'], batch['pairs']
            )
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Loss ‡∏£‡∏≤‡∏¢ Batch (‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢ Sample)
            batch_loss = 0
            num_spans_total = 0
            for b in range(len(batch['spans'])):
                if len(batch['spans'][b]) > 0:
                    num_real = ent_targets[b].shape[1]
                    curr_logits = ent_logits[b, :len(batch['spans'][b]), :num_real]
                    curr_targets = ent_targets[b][:, :num_real]
                    
                    l_ent = ent_criterion(curr_logits, curr_targets)
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ Loss ‡∏Ç‡∏≠‡∏á sample ‡∏ô‡∏µ‡πâ‡πÑ‡∏ß‡πâ
                    all_losses.append(l_ent.item() + 1e-6) # + epsilon ‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 0
                else:
                    all_losses.append(0.0) # ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏°‡πà‡∏°‡∏µ entity ‡πÄ‡∏•‡∏¢ (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô easy negative)

    # ‡πÅ‡∏õ‡∏•‡∏á Loss ‡πÄ‡∏õ‡πá‡∏ô Weights: ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà Loss ‡∏™‡∏π‡∏á‡∏à‡∏∞‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
    weights = torch.tensor(all_losses)
    # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (Normalization)
    weights = weights / weights.sum()
    
    return weights

def evaluate(model, dataloader, device, tokenizer, num_ent_labels):
    model.eval()
    
    # 1. ‡πÉ‡∏ä‡πâ Criterion ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô
    # ent_criterion = SigmoidFocalLoss(alpha=config.ALPHA, gamma=config.GAMMA, reduction='mean')
    # rel_criterion = SigmoidFocalLoss(alpha=config.ALPHA, gamma=config.GAMMA, reduction='none')


    ent_criterion = AsymmetricFocalLoss(alpha=config.ALPHA, gamma_pos=config.POS_GAMMA, gamma_neg=config.NEG_GAMMA, reduction='mean')
    rel_criterion = AsymmetricFocalLoss(alpha=config.ALPHA, gamma_pos=config.POS_GAMMA, gamma_neg=config.NEG_GAMMA, reduction='none')
    total_loss = 0
    correct_ent = 0
    total_ent = 0
    num_batches = 0 
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            text_ids = batch['text_ids'].to(device)
            text_mask = batch['text_mask'].to(device)
            
            (ent_lbl_ids, ent_lbl_mask), ent_targets, \
            (rel_lbl_ids, rel_lbl_mask), rel_targets = prepare_batch_inputs(batch, tokenizer, device)

            ent_logits, rel_logits = model(
                text_ids, text_mask, ent_lbl_ids, ent_lbl_mask,
                rel_lbl_ids, rel_lbl_mask, batch['spans'], batch['pairs']
            )
            
            batch_loss_ent = 0
            batch_loss_rel = 0
            valid_ent_samples = 0
            valid_rel_samples = 0
            
            for b in range(len(batch['spans'])):
                # --- Entity Val Loss & Acc ---
                if len(batch['spans'][b]) > 0:
                    num_real = ent_targets[b].shape[1]
                    curr_logits = ent_logits[b, :len(batch['spans'][b]), :num_real]
                    curr_targets = ent_targets[b][:, :num_real]
                    
                    # Focal Loss (One-hot)
                    batch_loss_ent += ent_criterion(curr_logits, curr_targets).item()
                    valid_ent_samples += 1
                    
                    # Accuracy (‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ Argmax ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ó‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™‡∏ñ‡∏π‡∏Å‡πÑ‡∏´‡∏°)
                    preds = curr_logits.argmax(dim=1)
                    target_indices = curr_targets.argmax(dim=1)
                    correct_ent += (preds == target_indices).sum().item()
                    total_ent += len(target_indices)

                # --- Relation Val Loss ---
                if rel_logits is not None and len(batch['pairs'][b]) > 0:
                    l_rel_raw = rel_criterion(rel_logits[b, :len(batch['pairs'][b]), :], rel_targets[b])
                    batch_loss_rel += l_rel_raw.mean().item()
                    valid_rel_samples += 1
            
            # ‡∏£‡∏ß‡∏° Loss ‡πÅ‡∏ö‡∏ö‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô
            if valid_ent_samples > 0 or valid_rel_samples > 0:
                l_e = (batch_loss_ent / valid_ent_samples) if valid_ent_samples > 0 else 0
                l_r = (batch_loss_rel / valid_rel_samples) * 1.5 if valid_rel_samples > 0 else 0
                total_loss += (l_e + l_r)
                num_batches += 1

    final_avg_loss = total_loss / num_batches if num_batches > 0 else 0
    accuracy = correct_ent / total_ent if total_ent > 0 else 0
    
    model.train()
    return final_avg_loss, accuracy


if __name__ == "__main__":
    # ==========================================
    # Main Training Script
    # ==========================================

    device = "cuda:2" if torch.cuda.is_available() else "cpu"
    print(f"Training on: {device}")

    # 1. Setup Data & Model
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Graph RAG
    full_dataset = GraphRAGDataset(
        json_file=config.TRAIN_FILE,
        tokenizer=tokenizer,
        max_len=256,
        neg_sample_ratio=0.0, # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ negative label sampling ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ labels ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        neg_span_ratio=2.0    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 2.0 - 2x negative spans ‡∏ï‡πà‡∏≠ positive spans
    )

    val_dataset_raw = GraphRAGDataset(
        json_file=config.VAL_FILE,  # ‡πÄ‡∏ä‡πà‡∏ô 'val_data.json'
        tokenizer=tokenizer,
        max_len=256,
        neg_sample_ratio=0.0,
        neg_span_ratio=2.0
    )

    train_set = full_dataset
    val_set = val_dataset_raw

    # # # üî• [NEW] ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: Train 90% / Val 10%
    # train_size = int(0.9 * len(full_dataset))
    # val_size = len(full_dataset) - train_size
    # train_set, val_set = random_split(full_dataset, [train_size, val_size])

    # print(f"üìä Data Split: Train {len(train_set)} / Val {len(val_set)}")

    # # ‡∏™‡∏£‡πâ‡∏≤‡∏á Loader ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
    # train_dataloader = DataLoader(
    #     train_set, 
    #     batch_size=config.BATCH_SIZE, 
    #     shuffle=True, 
    #     collate_fn=graph_rag_collate_fn
    # )

    # Validation ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Shuffle ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£ Batch Size ‡πÉ‡∏´‡∏ç‡πà‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö Gradient)
    val_dataloader = DataLoader(
        val_set, 
        batch_size=config.BATCH_SIZE * 2, 
        shuffle=False, 
        collate_fn=graph_rag_collate_fn
    )

    model = ZeroShotJointModel(config.MODEL_NAME).to(device)

    # # 2. Setup Optimizer & Loss
    # # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô CrossEntropyLoss ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single-label classification
    # # CrossEntropy ‡πÉ‡∏ä‡πâ softmax + log likelihood ‡∏ó‡∏≥‡πÉ‡∏´‡πâ model ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞ discriminate ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á classes
    # num_ent_labels = len(train_set.dataset.all_ent_labels_with_O)
    # class_weights = torch.ones(num_ent_labels).to(device)

    # class_weights[0] = 0.05   # üî• ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á O ‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 10%
    # class_weights[1:] = 5.0  # üî• ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á Entity ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß

    # # 2. ‡πÉ‡∏™‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Loss Function
    # ent_criterion = nn.CrossEntropyLoss(
    #     weight=class_weights, 
    #     reduction='mean',
    #     label_smoothing=0.0 # ‡∏õ‡∏¥‡∏î smoothing ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏°‡∏ä‡∏±‡∏î
    # )
    # ent_criterion = SigmoidFocalLoss(alpha=config.ALPHA, gamma=config.GAMMA, reduction='mean')  # Entities ‡πÄ‡∏õ‡πá‡∏ô single-label
    # rel_criterion = SigmoidFocalLoss(alpha=config.ALPHA, gamma=config.GAMMA, reduction='none')  # Relations can be multi-label

    ent_criterion = AsymmetricFocalLoss(alpha=config.ALPHA, gamma_pos=config.POS_GAMMA, gamma_neg=config.NEG_GAMMA, reduction='mean')  # Entities ‡πÄ‡∏õ‡πá‡∏ô single-label
    rel_criterion = AsymmetricFocalLoss(alpha=config.ALPHA, gamma_pos=config.POS_GAMMA, gamma_neg=config.NEG_GAMMA, reduction='none')  # Relations can be multi-label
    optimizer = optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)
    scaler = torch.amp.GradScaler('cuda')


    # --- Curriculum Configuration ---
    STAGES = [1,2,3] # 1: Easy, 2: Medium, 3: Hard (All data)
    EPOCHS_PER_STAGE = [1, 3, 3] # Total 7 epochs
    current_global_epoch = 0


    # # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Step ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # total_steps = len(train_dataloader) * config.NUM_EPOCHS

    # --- [NEW] ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Total Steps ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å Stages ---
    total_training_steps = 0
    for s_idx, s_val in enumerate(STAGES):
        # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Batch ‡πÉ‡∏ô Stage ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        s_indices = get_curriculum_indices(full_dataset, s_val)
        n_train = int(0.9 * len(s_indices))
        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô batches ‡∏ï‡πà‡∏≠ epoch = (n_train / batch_size) ‡∏õ‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        steps_in_stage = ((n_train + config.BATCH_SIZE - 1) // config.BATCH_SIZE) * EPOCHS_PER_STAGE[s_idx]
        total_training_steps += steps_in_stage

    print(f"Total planned training steps: {total_training_steps}")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Scheduler (Warmup 10% ‡πÅ‡∏£‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏•‡∏î LR ‡∏•‡∏á‡∏à‡∏ô‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_training_steps * 0.1), 
        num_training_steps=total_training_steps
    )
    # 3. Training Loop
    num_epochs = config.NUM_EPOCHS

    model.train()
    print(f"Start Training on {len(train_set)} samples...")


    best_val_acc = 0.0
    best_val_loss = float('inf')
    checkpoint_dir = config.OUTPUT_DIR
    os.makedirs(checkpoint_dir, exist_ok=True)

    for stage_idx, stage in enumerate(STAGES):
        print(f"\nüöÄ Entering Curriculum Stage {stage}: " + 
            ["Easy (Short)", "Medium (Normal)", "Hard (All/Rare)"][stage_idx])
        
        # Filter dataset for this stage
        stage_indices = get_curriculum_indices(full_dataset, stage)
        stage_subset = Subset(full_dataset, stage_indices)
        
        # Re-split for Train/Val
        train_size = int(0.9 * len(stage_subset))
        val_size = len(stage_subset) - train_size
        train_set, val_set = random_split(stage_subset, [train_size, val_size])

        # üî• ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stage 3 (Hard Stage)
        sampler = None
        if stage == 3:
            # 1. ‡∏£‡∏±‡∏ô Mining ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏Å‡πÉ‡∏ô Stage 3
            # ‡πÉ‡∏ä‡πâ DataLoader ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Weights
            temp_loader = DataLoader(train_set, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=graph_rag_collate_fn)
            sample_weights = get_sample_weights(model, temp_loader, device, tokenizer)
            
            # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Sampler: ‡∏à‡∏∞‡∏™‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏Å (Loss ‡∏™‡∏π‡∏á) ‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡πá‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏á‡πà‡∏≤‡∏¢
            sampler = WeightedRandomSampler(
                weights=sample_weights, 
                num_samples=len(sample_weights), 
                replacement=True # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏î‡∏¥‡∏°‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ (‡πÄ‡∏ô‡πâ‡∏ô‡∏¢‡πâ‡∏≥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î)
            )
        
        train_dataloader = DataLoader(train_set, batch_size=config.BATCH_SIZE,sampler=sampler, 
                                     collate_fn=graph_rag_collate_fn)
        
        print(f"üìä Stage {stage} Data: {len(train_set)} samples")


        for stage_epoch in range(EPOCHS_PER_STAGE[stage_idx]):
            total_loss = 0
            
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏à‡∏≤‡∏Å DataLoader ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á
            loop = tqdm(train_dataloader, desc=f"Epoch {stage_epoch+1}/{EPOCHS_PER_STAGE[stage_idx]}")
            for step, batch in enumerate(loop):
                
                # ‡∏¢‡πâ‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ GPU
                text_ids = batch['text_ids'].to(device)
                text_mask = batch['text_mask'].to(device)
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Labels ‡πÅ‡∏•‡∏∞ Targets (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Padding)
                (ent_lbl_ids, ent_lbl_mask), ent_targets, \
                (rel_lbl_ids, rel_lbl_mask), rel_targets = prepare_batch_inputs(batch, tokenizer, device)
                
                optimizer.zero_grad()
                
                with autocast():
                    # Forward Pass
                    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Model ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Label input ‡πÅ‡∏ö‡∏ö 3D [Batch, Num_Labels, Seq]
                    # ‡∏ñ‡πâ‡∏≤ Model ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ Model ‡πÉ‡∏´‡πâ Flatten ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Encoder
                    ent_logits, rel_logits = model(
                        text_ids, text_mask,
                        ent_lbl_ids, ent_lbl_mask,
                        rel_lbl_ids, rel_lbl_mask,
                        batch['spans'],
                        batch['pairs']
                    )
                    
                    # --- Calculate Loss (Custom Loop) ---
                    # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å ent_logits ‡πÅ‡∏•‡∏∞ targets ‡πÄ‡∏õ‡πá‡∏ô List of Tensors (‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏ï‡∏≤‡∏° Spans)
                    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Loss ‡∏ó‡∏µ‡∏•‡∏∞ Sample ‡πÉ‡∏ô Batch (‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Dynamic Data)
                    
                    loss_ent = 0
                    loss_rel = 0
                    valid_ent_samples = 0
                    valid_rel_samples = 0
                    
                    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏µ‡∏•‡∏∞ Sample ‡πÉ‡∏ô Batch
                    for b in range(len(batch['spans'])):
                        # --- 1. Entity Loss (Focal Loss) ---
                        if len(batch['spans'][b]) > 0:
                            num_real_labels = ent_targets[b].shape[1]
                            curr_ent_logits = ent_logits[b, :len(batch['spans'][b]), :num_real_labels]
                            
                            # üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ent_targets[b] ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô One-hot ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å Dataset 
                            # (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ argmax ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô CrossEntropy)
                            curr_ent_targets = ent_targets[b][:, :num_real_labels]
                            
                            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Focal Loss (‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤ 'mean' ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ)
                            l_ent = ent_criterion(curr_ent_logits, curr_ent_targets)
                            loss_ent += l_ent
                            valid_ent_samples += 1

                        # --- 2. Relation Loss (Focal Loss + Masking) ---
                        if rel_logits is not None and len(batch['pairs'][b]) > 0:
                            # logits: [Num_Pairs, Num_Rel_Labels]
                            curr_rel_logits = rel_logits[b, :len(batch['pairs'][b]), :]
                            curr_rel_targets = rel_targets[b]
                            
                            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏ö‡∏ö 'none' ‡∏à‡∏∞‡πÑ‡∏î‡πâ Tensor ‡∏Ç‡∏ô‡∏≤‡∏î [Num_Pairs, Num_Rel_Labels]
                            l_rel_raw = rel_criterion(curr_rel_logits, curr_rel_targets)
                            
                            # üéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Masking: 
                            # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Padding ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                            # ‡∏´‡∏≤‡∏Å dataset ‡∏™‡πà‡∏á‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà valid ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ .mean() ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
                            # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ô‡πâ‡∏ô Relation ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏•‡∏≤‡∏™‡∏ß‡πà‡∏≤‡∏á (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™ 0 ‡∏Ñ‡∏∑‡∏≠ No-Relation)
                            # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: l_rel = l_rel_raw.mean()
                            
                            loss_rel += l_rel_raw.mean()
                            valid_rel_samples += 1

                    # Average Loss
                    if valid_ent_samples > 0:
                        loss_ent = loss_ent / valid_ent_samples
                    else:
                        loss_ent = torch.tensor(0.0, device=device)

                    if valid_rel_samples > 0:
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡πâ Relation (‡πÄ‡∏ä‡πà‡∏ô 1.5 ‡∏´‡∏£‡∏∑‡∏≠ 2.0) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
                        loss_rel = (loss_rel / valid_rel_samples) * 1.5 
                    else:
                        loss_rel = torch.tensor(0.0, device=device)

                    # ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô Total Loss
                    loss = loss_ent + loss_rel
                    
                    # Handle case where both are zero (no valid samples at all)
                    if valid_ent_samples == 0 and valid_rel_samples == 0:
                        loss = torch.tensor(0.0, requires_grad=True, device=device)

                # Backward
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.MAX_GRAD_NORM)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                current_lr = optimizer.param_groups[0]['lr']
                loop.set_description(f"Epoch {stage_epoch+1}/{EPOCHS_PER_STAGE[stage_idx]} [LR: {current_lr:.2e}]")
                
                total_loss += loss.item()
                
                loop.set_postfix(loss=loss.item())


            
            avg_train_loss = total_loss / len(train_dataloader)
            print(f"--- Epoch {stage_epoch+1} Finished. Avg Loss: {avg_train_loss:.4f} ---")
            # üî• [NEW] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Validation
            print(f"\n--- Validating Epoch {stage_epoch+1} ---")
            num_ent_labels = len(full_dataset.all_ent_labels_with_O)
            val_loss, val_acc = evaluate(model, val_dataloader, device, tokenizer,num_ent_labels)
            
            print(f"‚úÖ Epoch {stage_epoch+1} Summary:")
            print(f"   - LR: {current_lr:.6f}")
            print(f"   - Train Loss: {avg_train_loss:.4f}")
            print(f"   - Val Loss:   {val_loss:.4f} (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
            print(f"   - Val Acc:    {val_acc*100:.2f}% (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
            print("-" * 40)

            # --- üî• [NEW] Best Model Checkpoint (Zero-shot Optimization) ---
            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ã‡∏ü‡πÄ‡∏°‡∏∑‡πà‡∏≠ Val Acc ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ Val Loss ‡∏ï‡πà‡∏≥‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            is_best = False
            
            # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 1: Accuracy ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏°‡∏≤
            if val_acc > best_val_acc:
                print(f"üåü New Best Acc! ({val_acc*100:.2f}% > {best_val_acc*100:.2f}%)")
                best_val_acc = val_acc
                is_best = True
            
            # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 2: Loss ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ä‡πà‡∏ß‡∏¢‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Generalization)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                # ‡∏ñ‡πâ‡∏≤ Loss ‡∏ï‡πà‡∏≥‡∏•‡∏á‡∏°‡∏≤‡∏Å ‡πÅ‡∏°‡πâ Acc ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏û‡∏∏‡πà‡∏á ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                if not is_best and val_acc > (best_val_acc * 0.95): # ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö Acc ‡∏ï‡∏Å‡πÑ‡∏î‡πâ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤ Loss ‡∏™‡∏ß‡∏¢
                     is_best = True

            if is_best:
                save_path = f"{checkpoint_dir}/best_model.bin"
                torch.save(model.state_dict(), save_path)
                
                # ‡πÄ‡∏ã‡∏ü Config ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                with open(f"{checkpoint_dir}/config.json", "w", encoding='utf-8') as f:
                    json.dump({
                        "model_name": config.MODEL_NAME,
                        "ent_labels": full_dataset.all_ent_labels_with_O,
                        "rel_labels": sorted(list(full_dataset.all_rel_labels)),
                        "best_epoch": stage_epoch + 1,
                        "stage": stage,
                        "val_acc": val_acc
                    }, f, ensure_ascii=False, indent=4)
                print(f"üíæ Saved Best Model to: {save_path}")
            
            # (Optional) Save Checkpoint here...

    print("Training Complete!")

    # --- Save Model ---
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    torch.save(model.state_dict(), f"{config.OUTPUT_DIR}/pytorch_model.bin")
    tokenizer.save_pretrained(config.OUTPUT_DIR)
    
    # Save Config - ‚úÖ ‡πÉ‡∏ä‡πâ all_ent_labels_with_O ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° "O" label
    with open(f"{config.OUTPUT_DIR}/config.json", "w", encoding='utf-8') as f:
        json.dump({
            "model_name": config.MODEL_NAME,
            "ent_labels": train_set.dataset.dataset.all_ent_labels_with_O,  # ‚úÖ ‡∏£‡∏ß‡∏° "O"
            "rel_labels": sorted(list(train_set.dataset.dataset.all_rel_labels))
        }, f, ensure_ascii=False, indent=4)
        
    print(f"Model saved to {config.OUTPUT_DIR}")
    print(f"‚úÖ Entity labels (with O): {train_set.dataset.dataset.all_ent_labels_with_O}")