import os
import sys
import json
# 1. ‡∏´‡∏≤ path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏°‡πà (NERRE)
# (__file__ ‡∏Ñ‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô -> dirname ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1 ‡πÑ‡∏î‡πâ 'train/' -> dirname ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2 ‡πÑ‡∏î‡πâ 'NERRE/')
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
sys.path.append(parent_dir)
import random
import numpy as np
from sklearn.metrics import f1_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast 
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# --- Imports ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ---
from model.model import ZeroShotJointModel 
import train_config as config
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import Subset
from torch.utils.data import random_split
from torch.utils.data import WeightedRandomSampler
import torch.optim.swa_utils as swa_utils
# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Dataset - ‡πÉ‡∏ä‡πâ GraphRAGDataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Graph RAG
from data.GraphRAGDataset import GraphRAGDataset, graph_rag_collate_fn

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå training data ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if not all(os.path.exists(f) for f in config.TRAIN_FILE):
    print(f"‚ùå Train file not found in: {config.TRAIN_FILE}")
    print("   Please create a training dataset first.")
    sys.exit(1)
else:
    print(f"‚úÖ Found training files: {config.TRAIN_FILE}")



# ==========================================
# Helper Function: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Label ‡∏ó‡∏µ‡πà‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
# ==========================================
def prepare_batch_inputs(batch, tokenizer, device):
    """
    ‡πÅ‡∏õ‡∏•‡∏á List of Lists of Strings (Labels) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Tensor ‡∏û‡∏£‡πâ‡∏≠‡∏° Padding
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Model ‡πÑ‡∏î‡πâ
    """
    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Entity Labels
    # ent_labels_text ‡πÄ‡∏õ‡πá‡∏ô List[List[str]] ‡πÄ‡∏ä‡πà‡∏ô [['Per', 'Org'], ['Loc']]
    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Tensor ‡∏Ç‡∏ô‡∏≤‡∏î [Batch, Max_Num_Labels, Seq_Len]
    
    batch_ent_labels = batch['ent_labels_text']
    
    # ‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Label ‡∏ó‡∏µ‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô Batch ‡∏ô‡∏µ‡πâ
    max_ent_labels = max(len(labels) for labels in batch_ent_labels)
    
    # Flatten ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Tokenize ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏£‡πá‡∏ß‡∏ß‡∏Å‡∏ß‡πà‡∏≤ loop)
    # ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ sample ‡∏°‡∏µ‡∏Å‡∏µ‡πà label
    flat_ent_labels = []
    for labels in batch_ent_labels:
        flat_ent_labels.extend(labels)
        # ‡πÄ‡∏ï‡∏¥‡∏° Dummy label ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö Max (Padding logic)
        flat_ent_labels.extend(["O"] * (max_ent_labels - len(labels)))

    # Tokenize
    ent_inputs = tokenizer(flat_ent_labels, return_tensors="pt", padding=True, truncation=True).to(device)
    
    # Reshape ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô [Batch, Max_Labels, Seq_Len]
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Model ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Label ‡∏Ç‡∏≠‡∏á Sample ‡πÑ‡∏´‡∏ô
    b = len(batch_ent_labels)
    seq_len = ent_inputs['input_ids'].shape[1]
    
    ent_label_ids = ent_inputs['input_ids'].view(b, max_ent_labels, seq_len)
    ent_label_mask = ent_inputs['attention_mask'].view(b, max_ent_labels, seq_len)
    
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Entity Targets (Padding)
    # ent_targets ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô List of Tensors [Spans, Num_Labels]
    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á Pad ‡πÉ‡∏´‡πâ Num_Labels ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö max_ent_labels
    padded_ent_targets = []
    for i, t in enumerate(batch['ent_targets']):
        # t shape: [Num_Spans, Num_Actual_Labels]
        # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£: [Num_Spans, Max_Ent_Labels]
        num_spans, num_actual = t.shape
        pad_size = max_ent_labels - num_actual
        
        if pad_size > 0:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡πà‡∏ô Zero ‡∏°‡∏≤‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
            padding = torch.zeros((num_spans, pad_size))
            t_padded = torch.cat([t, padding], dim=1)
        else:
            t_padded = t
        padded_ent_targets.append(t_padded.to(device))

    # --- ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö Relation (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ---
    # (‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡∏•‡∏∞‡πÑ‡∏ß‡πâ ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Entity)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ Relation Labels ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á Batch ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢
    rel_inputs = tokenizer(batch['rel_labels_text'][0], return_tensors="pt", padding=True, truncation=True).to(device)
    rel_label_ids = rel_inputs['input_ids'].unsqueeze(0).repeat(b, 1, 1) # Repeat ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤ Batch
    rel_label_mask = rel_inputs['attention_mask'].unsqueeze(0).repeat(b, 1, 1)
    
    # Pad Relation Targets (‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ Entity)
    padded_rel_targets = [t.to(device) for t in batch['rel_targets']]

    return (ent_label_ids, ent_label_mask), padded_ent_targets, \
           (rel_label_ids, rel_label_mask), padded_rel_targets


# Inside your GraphRAGDataset or as a wrapper
def get_curriculum_indices(dataset, stage):
    indices = []
    for i in range(len(dataset)):
        sample = dataset.data[i]
        text_len = len(sample['text'].split())
        num_ents = len(sample['entities'])
        num_rels = len(sample.get('relations', []))

        if stage == 1: # Easy but Meaningful
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏î‡∏µ‡πÜ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏•‡∏≠‡∏¢‡πÜ)
            if 10 < text_len < 35 and 2 <= num_ents <= 3 and num_rels >= 1:
                indices.append(i)
        elif stage == 2: # Medium - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
            if text_len < 60 and num_ents <= 6:
                indices.append(i)
        else: # Stage 3: All data (Hard / Rare cases)
            indices.append(i)
            
    # ‡∏ñ‡πâ‡∏≤ Stage 1 ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡πÄ‡∏ä‡πà‡∏ô < 5000) ‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏™‡∏±‡πâ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏°‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°
    if stage == 1 and len(indices) < 5000:
        return list(range(len(dataset)))[:10000] 
        
    return indices

def set_trainable_layers(model, stage):
    if stage == 1:
        # Stage 1: Mastering Entity
        # Train: Encoder, Entity Projector, Width Embedding, Span Attention
        # Freeze: Relation Projector, Distance Embedding, Directional Gates
        print("üîì [Stage 1] Training Encoder + Entity Layout (Relation logic Frozen)")
        for param in model.parameters():
            param.requires_grad = True
            
        # Freeze Relation specific components
        for param in model.relation_proj.parameters():
            param.requires_grad = False
        if hasattr(model, 'dist_emb'):
            for param in model.dist_emb.parameters():
                param.requires_grad = False
        if hasattr(model, 's_gate'):
            for param in model.s_gate.parameters():
                param.requires_grad = False
        if hasattr(model, 'o_gate'):
            for param in model.o_gate.parameters():
                param.requires_grad = False
            
    elif stage == 2:
        # Stage 2: Relation Injection 
        # Freeze: Encoder, Entity Projector, Width Embedding
        # Train: Relation Projector, Distance, Gates, Temps
        print("üîí [Stage 2] Freezing Encoder/Entity (Training Relation Logic ONLY)")
        for param in model.parameters():
            param.requires_grad = False
            
        # Unfreeze Relation components
        for param in model.relation_proj.parameters():
            param.requires_grad = True
        if hasattr(model, 'dist_emb'):
            for param in model.dist_emb.parameters():
                param.requires_grad = True
        if hasattr(model, 's_gate'):
            for param in model.s_gate.parameters():
                param.requires_grad = True
        if hasattr(model, 'o_gate'):
            for param in model.o_gate.parameters():
                param.requires_grad = True
        
        # Allow Temperatures to tune
        if hasattr(model, 'log_temp_ent'): model.log_temp_ent.requires_grad = True
        if hasattr(model, 'log_temp_rel'): model.log_temp_rel.requires_grad = True

    else:
        # Stage 3: Joint Fine-tuning
        print("üîì [Stage 3] Joint Fine-tuning (All Layers Trainable)")
        for param in model.parameters():
            param.requires_grad = True

def get_sample_weights(model, dataloader, device, tokenizer):
    model.eval()
    all_losses = []
    
    # ‡πÉ‡∏ä‡πâ CrossEntropyLoss ‡πÅ‡∏ö‡∏ö‡∏•‡∏î‡∏ó‡∏≠‡∏ô (None reduction) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Loss ‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
    ent_criterion = nn.CrossEntropyLoss(reduction='none')

    print("üîç Mining Hard Negatives (Calculating Sample Weights)...")
    with torch.no_grad():
        for batch in tqdm(dataloader):
            text_ids = batch['text_ids'].to(device)
            text_mask = batch['text_mask'].to(device)
            
            (ent_lbl_ids, ent_lbl_mask), ent_targets, \
            (rel_lbl_ids, rel_lbl_mask), rel_targets = prepare_batch_inputs(batch, tokenizer, device)

            # ‡∏£‡∏±‡∏ô Forward ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Logits
            ent_logits, _ = model(
                text_ids, text_mask, ent_lbl_ids, ent_lbl_mask,
                rel_lbl_ids, rel_lbl_mask, batch['spans'], batch['pairs']
            )
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Loss ‡∏£‡∏≤‡∏¢ Batch (‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢ Sample)
            batch_loss = 0
            num_spans_total = 0
            for b in range(len(batch['spans'])):
                if len(batch['spans'][b]) > 0:
                    num_real = ent_targets[b].shape[1]
                    curr_logits = ent_logits[b, :len(batch['spans'][b]), :num_real]
                    curr_targets = ent_targets[b][:, :num_real]
                    
                    # Convert to Indices
                    curr_target_indices = curr_targets.argmax(dim=1)
                    
                    l_ent = ent_criterion(curr_logits, curr_target_indices)
                    
                    # Mean Loss of this sample
                    loss_val = l_ent.mean().item()
                    all_losses.append(loss_val)
                else:
                    # ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏°‡πà‡∏°‡∏µ entity ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πà‡∏≥‡πÜ ‡πÑ‡∏ß‡πâ (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0)
                    all_losses.append(0.01)

    # ‡πÅ‡∏õ‡∏•‡∏á Loss ‡πÄ‡∏õ‡πá‡∏ô Weights: ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà Loss ‡∏™‡∏π‡∏á‡∏à‡∏∞‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
    weights = torch.tensor(all_losses)
    # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (Normalization)
    weights = weights / weights.sum()
    
    return weights

def evaluate(model, dataloader, device, tokenizer, num_ent_labels):
    model.eval()
    
    # 1. Setup Criterions
    # Entity: CrossEntropyLoss (matching training)
    ent_criterion = nn.CrossEntropyLoss(reduction='mean')
    
    # Relation: CrossEntropyLoss (updated)
    rel_criterion = nn.CrossEntropyLoss(reduction='mean')
    
    total_loss = 0
    correct_ent, total_ent = 0, 0
    
    # Store all relation preds and targets for F1 calculation
    all_rel_preds = []
    all_rel_targets = []
    
    num_batches = 0 

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            text_ids = batch['text_ids'].to(device)
            text_mask = batch['text_mask'].to(device)
            
            (ent_lbl_ids, ent_lbl_mask), ent_targets, \
            (rel_lbl_ids, rel_lbl_mask), rel_targets = prepare_batch_inputs(batch, tokenizer, device)

            # Forward pass
            ent_logits, rel_logits = model(
                text_ids, text_mask, ent_lbl_ids, ent_lbl_mask,
                rel_lbl_ids, rel_lbl_mask, batch['spans'], batch['pairs']
            )
            
            batch_loss = 0
            valid_samples_in_batch = 0
            
            for b in range(len(batch['spans'])):
                sample_loss = 0
                has_valid_task = False

                # --- 1. Entity Metrics & Loss ---
                if len(batch['spans'][b]) > 0:
                    num_real = ent_targets[b].shape[1]
                    curr_logits = ent_logits[b, :len(batch['spans'][b]), :num_real]
                    curr_targets = ent_targets[b][:, :num_real]
                    
                    # Convert to Indices for CrossEntropy
                    curr_target_indices = curr_targets.argmax(dim=1)

                    # Calculate Entity Loss
                    l_ent = ent_criterion(curr_logits, curr_target_indices)
                    sample_loss += l_ent
                    
                    # Accuracy
                    preds = curr_logits.argmax(dim=1)
                    correct_ent += (preds == curr_target_indices).sum().item()
                    total_ent += len(curr_target_indices)
                    has_valid_task = True

                # --- 2. Relation Metrics & Loss ---
                if rel_logits is not None and len(batch['pairs'][b]) > 0:
                    curr_rel_logits = rel_logits[b, :len(batch['pairs'][b]), :]
                    curr_rel_targets = rel_targets[b]
                    
                    # Convert to Indices
                    curr_rel_targets_idx = curr_rel_targets.argmax(dim=1)
                    
                    l_rel = rel_criterion(curr_rel_logits, curr_rel_targets_idx)
                    
                    sample_loss += l_rel

                    # Store for global F1
                    rel_preds = curr_rel_logits.argmax(dim=-1)
                    all_rel_preds.extend(rel_preds.cpu().numpy())
                    all_rel_targets.extend(curr_rel_targets_idx.cpu().numpy())
                    
                    has_valid_task = True
                
                if has_valid_task:
                    batch_loss += sample_loss
                    valid_samples_in_batch += 1

            if valid_samples_in_batch > 0:
                total_loss += (batch_loss / valid_samples_in_batch).item()
                num_batches += 1

    # Final Aggregation
    final_avg_loss = total_loss / num_batches if num_batches > 0 else 0
    ent_acc = correct_ent / total_ent if total_ent > 0 else 0
    
    # Calculate Relation F1 (Macro/Micro) excluding NO_RELATION (index 0)
    # Assuming index 0 is NO_RELATION
    if len(all_rel_targets) > 0:
        # labels=list(range(1, max(all_rel_targets)+1)) if using dynamics, 
        # but let's just use unique labels present in data minus 0
        unique_labels = list(set(all_rel_targets) | set(all_rel_preds))
        if 0 in unique_labels: unique_labels.remove(0)
        
        rel_f1 = f1_score(all_rel_targets, all_rel_preds, labels=unique_labels, average='micro', zero_division=0)
        rel_acc = (np.array(all_rel_preds) == np.array(all_rel_targets)).mean() # Raw accuracy
    else:
        rel_f1 = 0.0
        rel_acc = 0.0
    
    print(f"\nüìä Validation Results:")
    print(f"   - Entity Acc:   {ent_acc*100:.2f}%")
    print(f"   - Relation Acc: {rel_acc*100:.2f}% (Includes NO_RELATION)")
    print(f"   - Relation F1:  {rel_f1*100:.2f}% (Micro, Excludes NO_RELATION)")
    print(f"   - Total Loss:   {final_avg_loss:.4f}")

    model.train()
    
    # üî• [User Request] Combined Score for Best Model
    # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (Entity Accuracy + Relation F1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
    combined_score = (ent_acc + rel_f1) / 2
    
    return final_avg_loss, combined_score


if __name__ == "__main__":
    # ==========================================
    # Main Training Script
    # ==========================================

    device = "cuda:2" if torch.cuda.is_available() else "cpu"
    print(f"Training on: {device}")

    # 1. Setup Data & Model
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)


    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Graph RAG
    full_dataset = GraphRAGDataset(
        json_file=config.TRAIN_FILE,
        tokenizer=tokenizer,
        max_len=256,
        neg_sample_ratio=0.0, # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ negative label sampling ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ labels ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        neg_span_ratio=6.0    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 2.0 - 2x negative spans ‡∏ï‡πà‡∏≠ positive spans
    )

    val_dataset_raw = GraphRAGDataset(
        json_file=config.VAL_FILE,
        tokenizer=tokenizer,
        max_len=256,
        neg_sample_ratio=0.0,
        neg_span_ratio=2.0
    )

    train_set = full_dataset
    val_set = val_dataset_raw

    # Validation ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Shuffle ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£ Batch Size ‡πÉ‡∏´‡∏ç‡πà‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö Gradient)
    val_dataloader = DataLoader(
        val_set, 
        batch_size=config.BATCH_SIZE * 2, 
        shuffle=False, 
        collate_fn=graph_rag_collate_fn
    )

    model = ZeroShotJointModel(config.MODEL_NAME).to(device)

    if hasattr(train_set, 'dataset'):
        all_labels = train_set.dataset.all_ent_labels_with_O
        all_rel_labels = train_set.dataset.all_rel_labels_with_NO_REL
    else:
        all_labels = train_set.all_ent_labels_with_O
        all_rel_labels = train_set.all_rel_labels_with_NO_REL
        
    num_ent_labels = len(all_labels)
    class_weights = torch.ones(num_ent_labels).to(device)
    class_weights[0] = 0.1  # ‚úÖ Further increased O weight to 0.5 to improve Precision
    class_weights[1:] = 2.0 # Keep Real Entities higher
    
    ent_criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')

    # üî• [SOTA Rel Fix] Switch Relation to CrossEntropy too
    num_rel_labels = len(all_rel_labels)
    rel_class_weights = torch.ones(num_rel_labels).to(device)
    rel_class_weights[0] = 0.8   # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å NO_RELATION ‡πÄ‡∏õ‡πá‡∏ô 0.8 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏î False Positives (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏°‡∏±‡πà‡∏ß)
    
    rel_criterion = nn.CrossEntropyLoss(weight=rel_class_weights, reduction='mean', label_smoothing=0.05) 
    
    optimizer = optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)
    scaler = torch.amp.GradScaler('cuda')


    # --- Curriculum Configuration ---
    STAGES = [1,2,3] # 1: Easy, 2: Medium, 3: Hard (All data)
    EPOCHS_PER_STAGE = [3, 5, 7] # Total 15 epochs
    current_global_epoch = 0


    # --- [SWA Config] ---
    swa_model = swa_utils.AveragedModel(model)
    # SWA Learning Rate ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÜ (‡πÄ‡∏ä‡πà‡∏ô 10% ‡∏Ç‡∏≠‡∏á LR ‡∏õ‡∏Å‡∏ï‡∏¥)
    swa_scheduler = swa_utils.SWALR(optimizer, swa_lr=config.LR * 0.1)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏° SWA: ‡πÉ‡∏ô Stage 3 (Epoch 11-12 ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 12)
    # ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô 2 Epoch ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Stage 3
    swa_start_epoch = EPOCHS_PER_STAGE[2] - 2

    # 3. Training Loop
    num_epochs = config.NUM_EPOCHS

    model.train()
    print(f"Start Training on {len(train_set)} samples...")


    best_val_acc = 0.0
    best_val_loss = float('inf')
    checkpoint_dir = config.OUTPUT_DIR
    os.makedirs(checkpoint_dir, exist_ok=True)

    for stage_idx, stage in enumerate(STAGES):
        print(f"\nüöÄ Entering Curriculum Stage {stage}: " + 
            ["Easy (Short)", "Medium (Normal)", "Hard (All/Rare)"][stage_idx])
        
        # if os.path.exists(f"{config.OUTPUT_DIR}/best_model.bin"):
        #     model.load_state_dict(torch.load(f"{config.OUTPUT_DIR}/best_model.bin", map_location=device))
        #     print("‚úÖ Loaded previous best weights. Ready for next stage!")
        
        set_trainable_layers(model, stage)


        # Filter dataset for this stage
        stage_indices = get_curriculum_indices(full_dataset, stage)
        stage_subset = Subset(full_dataset, stage_indices)
        
        # Re-split for Train/Val
        train_size = int(1 * len(stage_subset))
        val_size = len(stage_subset) - train_size
        train_set, val_set = random_split(stage_subset, [train_size, val_size])



        # --- üöÄ [V19 FIX] RESET OPTIMIZER & SCHEDULER FOR NEW STAGE ---
        # ‡∏Å‡∏≤‡∏£ Reset ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ Stage 2 ‡πÅ‡∏•‡∏∞ 3 ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ LR ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (Warmup)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Steps ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Stage ‡∏ô‡∏µ‡πâ
        num_epochs_this_stage = EPOCHS_PER_STAGE[stage_idx]
        steps_per_epoch = (train_size + config.BATCH_SIZE - 1) // config.BATCH_SIZE
        total_stage_steps = steps_per_epoch * num_epochs_this_stage
        current_lr = config.LR if stage < 3 else config.LR * 0.2
        # Re-initialize Optimizer (‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ momentum/velocity ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏Ñ‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å Stage ‡∏Å‡πà‡∏≠‡∏ô)
        trainable_params = filter(lambda p: p.requires_grad, model.parameters())
        optimizer = optim.AdamW(trainable_params, lr=current_lr, weight_decay=config.WEIGHT_DECAY)
        
        # Re-create Scheduler ‡πÉ‡∏´‡πâ‡∏°‡∏µ Warmup ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stage ‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        # ‡πÉ‡∏ä‡πâ 10-15% ‡∏Ç‡∏≠‡∏á steps ‡πÉ‡∏ô stage ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô warmup
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(total_stage_steps * 0.1), 
            num_training_steps=total_stage_steps
        )

        # üî• ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stage 3 (Hard Stage)
        sampler = None
        if stage == 3:
            # 1. ‡∏´‡∏≤ Weights ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            temp_loader = DataLoader(train_set, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=graph_rag_collate_fn)
            sample_weights = get_sample_weights(model, temp_loader, device, tokenizer)
            
            # 2. üî• [Step 2] Active Hard Sampling Logic
            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á 2 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏¥‡∏ô‡πÄ‡∏î‡πá‡∏Å‡∏ã‡πå
            num_samples = len(train_set)
            
            # - ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà 1: Top 25% ‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà Loss ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏™‡∏∞‡∏™‡∏°)
            num_hard = int(num_samples * 0.25)
            top_hard_indices = torch.topk(sample_weights, k=num_hard).indices.tolist()
            
            # - ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà 2: ‡∏™‡∏∏‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
            all_indices = list(range(num_samples))
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Weighted Sampler ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô '‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô' ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©
            # ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏∏‡πà‡∏° Top Hard
            active_weights = sample_weights.clone()
            active_weights[top_hard_indices] *= 2.0 # ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ú‡∏¥‡∏î‡∏ã‡πâ‡∏≥‡∏ã‡∏≤‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏° 2 ‡πÄ‡∏ó‡πà‡∏≤
            
            sampler = WeightedRandomSampler(
                weights=active_weights, 
                num_samples=num_samples, 
                replacement=True 
            )
            
            train_dataloader = DataLoader(train_set, batch_size=config.BATCH_SIZE, sampler=sampler, collate_fn=graph_rag_collate_fn)
        else:
            train_dataloader = DataLoader(train_set, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=graph_rag_collate_fn)
        print(f"üìä Stage {stage} Data: {len(train_set)} samples")


        for stage_epoch in range(EPOCHS_PER_STAGE[stage_idx]):


            # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Stage 3 ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô Epoch ‡∏ó‡∏µ‡πà 1, 3, 5... ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡∏°‡πà
            if stage == 3 and (stage_epoch % 2 == 0):
                print(f"\nüîÑ Stage 3: Re-calculating Sample Weights for Epoch {stage_epoch+1}...")
                
                # ‡πÉ‡∏ä‡πâ DataLoader ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏• '‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ' ‡∏¢‡∏±‡∏á‡∏ó‡∏≤‡∏¢‡∏ú‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà
                temp_loader = DataLoader(train_set, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=graph_rag_collate_fn)
                sample_weights = get_sample_weights(model, temp_loader, device, tokenizer)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Sampler ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                sampler = WeightedRandomSampler(
                    weights=sample_weights, 
                    num_samples=len(sample_weights), 
                    replacement=True
                )
                
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï DataLoader ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô
                train_dataloader = DataLoader(
                    train_set, 
                    batch_size=config.BATCH_SIZE, 
                    sampler=sampler, 
                    collate_fn=graph_rag_collate_fn
                )
                print("‚úÖ DataLoader updated with fresh hard samples!")

            # ‡∏Å‡∏£‡∏ì‡∏µ Stage ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏´‡∏£‡∏∑‡∏≠ Epoch ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Re-sample ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DataLoader ‡πÄ‡∏î‡∏¥‡∏°
            elif stage_epoch == 0: # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stage 1 ‡πÅ‡∏•‡∏∞ 2
                 train_dataloader = DataLoader(
                    train_set, 
                    batch_size=config.BATCH_SIZE, 
                    shuffle=True, 
                    collate_fn=graph_rag_collate_fn
                )




            total_loss = 0
            
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏à‡∏≤‡∏Å DataLoader ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á
            loop = tqdm(train_dataloader, desc=f"Epoch {stage_epoch+1}/{EPOCHS_PER_STAGE[stage_idx]}")
            for step, batch in enumerate(loop):
                
                # ‡∏¢‡πâ‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ GPU
                text_ids = batch['text_ids'].to(device)
                text_mask = batch['text_mask'].to(device)
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Labels ‡πÅ‡∏•‡∏∞ Targets (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Padding)
                (ent_lbl_ids, ent_lbl_mask), ent_targets, \
                (rel_lbl_ids, rel_lbl_mask), rel_targets = prepare_batch_inputs(batch, tokenizer, device)
                
                # optimizer.zero_grad()
                
                with autocast():
                    # Forward Pass
                    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Model ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Label input ‡πÅ‡∏ö‡∏ö 3D [Batch, Num_Labels, Seq]
                    # ‡∏ñ‡πâ‡∏≤ Model ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ Model ‡πÉ‡∏´‡πâ Flatten ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Encoder
                    ent_logits, rel_logits = model(
                        text_ids, text_mask,
                        ent_lbl_ids, ent_lbl_mask,
                        rel_lbl_ids, rel_lbl_mask,
                        batch['spans'],
                        batch['pairs']
                    )
                    
                    # --- Calculate Loss (Custom Loop) ---
                    # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å ent_logits ‡πÅ‡∏•‡∏∞ targets ‡πÄ‡∏õ‡πá‡∏ô List of Tensors (‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏ï‡∏≤‡∏° Spans)
                    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Loss ‡∏ó‡∏µ‡∏•‡∏∞ Sample ‡πÉ‡∏ô Batch (‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Dynamic Data)
                    
                    loss_ent = 0
                    loss_rel = 0
                    valid_ent_samples = 0
                    valid_rel_samples = 0
                    
                    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏µ‡∏•‡∏∞ Sample ‡πÉ‡∏ô Batch
                    for b in range(len(batch['spans'])):
                        # --- 1. Entity Loss (CrossEntropy) ---
                        if len(batch['spans'][b]) > 0:
                            num_real_labels = ent_targets[b].shape[1]
                            curr_ent_logits = ent_logits[b, :len(batch['spans'][b]), :num_real_labels]
                            
                            # ‡πÅ‡∏õ‡∏•‡∏á One-hot ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Class Indices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CrossEntropyLoss
                            curr_ent_targets_onehot = ent_targets[b][:, :num_real_labels]
                            curr_ent_targets_idx = curr_ent_targets_onehot.argmax(dim=1)
                            
                            l_ent = ent_criterion(curr_ent_logits, curr_ent_targets_idx)
                            loss_ent += l_ent
                            valid_ent_samples += 1

                        # --- 2. Relation Loss (CrossEntropy) ---
                        if rel_logits is not None and len(batch['pairs'][b]) > 0:
                            # logits: [Num_Pairs, Num_Rel_Labels]
                            curr_rel_logits = rel_logits[b, :len(batch['pairs'][b]), :]
                            curr_rel_targets = rel_targets[b]
                            
                            # Convert to Indices
                            curr_rel_targets_idx = curr_rel_targets.argmax(dim=1)
                            
                            l_rel = rel_criterion(curr_rel_logits, curr_rel_targets_idx)
                            
                            loss_rel += l_rel
                            valid_rel_samples += 1

                    # Average Loss
                    if valid_ent_samples > 0:
                        loss_ent = loss_ent / valid_ent_samples
                    else:
                        loss_ent = torch.tensor(0.0, requires_grad=True, device=device)

                    if valid_rel_samples > 0:
                        loss_rel = loss_rel / valid_rel_samples
                    else:
                        loss_rel = torch.tensor(0.0, requires_grad=True, device=device)

                    # # ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô Total Loss
                    # loss = (loss_ent) + (loss_rel)

                    # 3. üî• ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏° Loss ‡∏ï‡∏≤‡∏° Stage
                    if stage == 1:
                        # ‡πÄ‡∏ô‡πâ‡∏ô Entity ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πâ‡∏≤ 80%
                        loss = loss_ent * 1.0 
                    elif stage == 2:
                        # ‡∏™‡∏≠‡∏ô RE ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ NER ‡∏ñ‡∏π‡∏Å Freeze ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß)
                        # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏Ñ‡∏π‡∏ì‡∏™‡∏π‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Gradient ‡πÅ‡∏£‡∏á‡∏û‡∏≠‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏´‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà
                        loss = loss_rel * 50.0 
                    else:
                        # ‡∏à‡∏π‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô (Balance)
                        loss = (loss_ent * 1.0) + (loss_rel * 2.0)
                    
                    # Handle case where both are zero (no valid samples at all)
                    if valid_ent_samples == 0 and valid_rel_samples == 0:
                        loss = torch.tensor(0.0, requires_grad=True, device=device)

                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏•‡∏¢‡πÉ‡∏ô Batch ‡∏ô‡∏±‡πâ‡∏ô
                    if valid_ent_samples == 0 and valid_rel_samples == 0:
                        loss = torch.tensor(0.0, requires_grad=True, device=device)

                # Backward
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.MAX_GRAD_NORM)
                scaler.step(optimizer)
                scaler.update()

                # --- üî• [V3 SWA Upgrade] ‡∏ß‡∏≤‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ üî• ---
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Stage 3 ‡πÅ‡∏•‡∏∞‡∏ñ‡∏∂‡∏á Epoch ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏° SWA ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
                if stage == 3 and stage_epoch >= swa_start_epoch:
                    # 1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏∞‡∏™‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤ swa_model)
                    swa_model.update_parameters(model)
                    # 2. ‡πÉ‡∏ä‡πâ swa_scheduler (‡∏Ñ‡πà‡∏≤ LR ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÜ) ‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏Å
                    swa_scheduler.step()
                else:
                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (Stage 1, 2 ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏ô Stage 3) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Linear Scheduler ‡∏õ‡∏Å‡∏ï‡∏¥
                    scheduler.step()
                
                # ---------------------------------------

                optimizer.zero_grad() # ‡∏•‡πâ‡∏≤‡∏á gradient ‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÅ‡∏•‡πâ‡∏ß

                current_lr = optimizer.param_groups[0]['lr']
                loop.set_description(f"Epoch {stage_epoch+1}/{EPOCHS_PER_STAGE[stage_idx]} [LR: {current_lr:.2e}]")
                
                total_loss += loss.item()
                
                # loop.set_postfix(loss=loss.item())
                loop.set_postfix(
                    loss=f"{loss.item():.3f}",
                    ent_loss=f"{loss_ent.item():.3f}",
                    rel_loss=f"{loss_rel.item():.3f}" 
                )


            
            avg_train_loss = total_loss / len(train_dataloader)
            print(f"--- Epoch {stage_epoch+1} Finished. Avg Loss: {avg_train_loss:.4f} ---")
            # üî• [NEW] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Validation
            print(f"\n--- Validating Epoch {stage_epoch+1} ---")
            num_ent_labels = len(full_dataset.all_ent_labels_with_O)
            val_loss, val_acc = evaluate(model, val_dataloader, device, tokenizer,num_ent_labels)
            
            print(f"‚úÖ Epoch {stage_epoch+1} Summary:")
            print(f"   - LR: {current_lr:.6f}")
            print(f"   - Train Loss: {avg_train_loss:.4f}")
            print(f"   - Val Loss:   {val_loss:.4f} (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)")
            print(f"   - Val Score:  {val_acc*100:.2f}% (Average of Ent Acc & Rel F1)")
            print("-" * 40)

            # --- üî• [NEW] Best Model Checkpoint (Zero-shot Optimization) ---
            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ã‡∏ü‡πÄ‡∏°‡∏∑‡πà‡∏≠ combined score ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô
            is_best = False
            
            # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 1: Score ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏°‡∏≤
            if val_acc > best_val_acc:
                print(f"üåü New Best Score! ({val_acc*100:.2f}% > {best_val_acc*100:.2f}%)")
                best_val_acc = val_acc
                is_best = True
            
            # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà 2: Loss ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ä‡πà‡∏ß‡∏¢‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Generalization)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                # ‡∏ñ‡πâ‡∏≤ Loss ‡∏ï‡πà‡∏≥‡∏•‡∏á‡∏°‡∏≤‡∏Å ‡πÅ‡∏°‡πâ Acc ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏û‡∏∏‡πà‡∏á ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                if not is_best and val_acc > (best_val_acc * 0.95): # ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö Acc ‡∏ï‡∏Å‡πÑ‡∏î‡πâ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤ Loss ‡∏™‡∏ß‡∏¢
                     is_best = True

            if is_best:
                save_path = f"{checkpoint_dir}/best_model.bin"
                torch.save(model.state_dict(), save_path)
                
                # ‡πÄ‡∏ã‡∏ü Config ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                with open(f"{checkpoint_dir}/config.json", "w", encoding='utf-8') as f:
                    json.dump({
                        "model_name": config.MODEL_NAME,
                        "ent_labels": full_dataset.all_ent_labels_with_O,
                        "rel_labels": sorted(list(full_dataset.all_rel_labels)),
                        "best_epoch": stage_epoch + 1,
                        "stage": stage,
                        "val_acc": val_acc
                    }, f, ensure_ascii=False, indent=4)
                print(f"üíæ Saved Best Model to: {save_path}")
            
            # (Optional) Save Checkpoint here...


    # --- üî• [V3 SWA Finalize] ‡∏ß‡∏≤‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ (‡∏ô‡∏≠‡∏Å‡∏•‡∏π‡∏õ Stage ‡πÉ‡∏´‡∏ç‡πà) üî• ---
    # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å Stage (‡∏à‡∏ö Stage 3)
    
    print("\nüöÄ All Stages Complete. Finalizing SWA Model...")
    
    # 1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï BatchNorm/LayerNorm statistics
    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å SWA ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô
    swa_utils.update_bn(train_dataloader, swa_model, device=device)

    # 2. ‡πÉ‡∏ä‡πâ swa_model ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Evaluate ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå 90%+
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: swa_model ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏´‡πà‡∏≠ (wrap) ‡πÑ‡∏ß‡πâ ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ DataParallel ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á .module ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    model_to_eval = swa_model.module if hasattr(swa_model, 'module') else swa_model
    
    # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á BatchNorm update ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô evaluate
    val_loss, val_acc = evaluate(model_to_eval, val_dataloader, device, tokenizer, num_ent_labels)
    
    print(f"üìä Final SWA Validation Accuracy: {val_acc*100:.2f}%")

    # 3. ‡πÄ‡∏ã‡∏ü SWA Model ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    torch.save(swa_model.state_dict(), f"{config.OUTPUT_DIR}/swa_model.bin")
    
    # -------------------------------------------------------



    print("Training Complete!")

    # --- Save Model ---
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    torch.save(model.state_dict(), f"{config.OUTPUT_DIR}/pytorch_model.bin")
    tokenizer.save_pretrained(config.OUTPUT_DIR)
    
    # Find base dataset
    base_ds = train_set
    while hasattr(base_ds, 'dataset'):
        base_ds = base_ds.dataset

    # Save Config - ‚úÖ ‡πÉ‡∏ä‡πâ all_ent_labels_with_O ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° "O" label
    # Also save descriptions for inference
    with open(f"{config.OUTPUT_DIR}/config.json", "w", encoding='utf-8') as f:
        json.dump({
            "model_name": config.MODEL_NAME,
            "ent_labels": base_ds.all_ent_labels_with_O,  # ‚úÖ ‡∏£‡∏ß‡∏° "O"
            "rel_labels": base_ds.all_rel_labels_with_NO_REL,
            "ent_label_descriptions": getattr(base_ds, 'ent_label_texts', []),
            "rel_label_descriptions": getattr(base_ds, 'rel_label_texts', []),
            "max_len": 256
        }, f, ensure_ascii=False, indent=4)
        
    print(f"Model saved to {config.OUTPUT_DIR}")
    print(f"‚úÖ Entity labels (with O): {base_ds.all_ent_labels_with_O}")