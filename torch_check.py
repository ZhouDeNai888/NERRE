import torch
import transformers

print("-" * 30)
print(f"PyTorch Version: {torch.__version__}")
print(f"Transformers Version: {transformers.__version__}")
print("-" * 30)

if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    print(f"‚úÖ CUDA is available! Found {device_count} GPU(s).")
    for i in range(device_count):
        print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô A100 ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏° ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö BF16 (Bfloat16) ‡πÑ‡∏´‡∏°
        if torch.cuda.is_bf16_supported():
            print("   üöÄ BFloat16 is supported! (Great for A100)")
else:
    print("‚ùå CUDA is NOT available. You are running on CPU.")