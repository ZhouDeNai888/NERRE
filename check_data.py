import json
from transformers import AutoTokenizer
from data.GraphRAGDataset import GraphRAGDataset # Import class Dataset ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì



# Config
MODEL_NAME = "xlm-roberta-base" # ‡∏´‡∏£‡∏∑‡∏≠ Large ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ
DATA_FILE = "dataset/multilingual_data_v5_10000.json"

print("‚è≥ Loading Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

print("üìÇ Loading Dataset...")
# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏∏‡πà‡∏° Negative (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÜ)
ds = GraphRAGDataset(DATA_FILE, tokenizer, neg_span_ratio=0.0)

print(f"\n--- üîç Validating {len(ds)} samples ---")
total_entities_json = 0
total_entities_valid = 0

# ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏±‡∏Å 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏´‡∏°‡∏î‡πÄ‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ
for i in range(min(len(ds), 1000)):
    item = ds.data[i]
    raw_entities_count = len(item.get('entities', []))
    total_entities_json += raw_entities_count
    
    # ‡∏î‡∏π‡∏ß‡πà‡∏≤ Dataset Class ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Token ‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà‡∏ï‡∏±‡∏ß
    processed_item = ds[i]
    valid_count = processed_item['num_positive_spans']
    total_entities_valid += valid_count
    
    # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÉ‡∏´‡πâ‡πÇ‡∏ß‡∏¢‡∏ß‡∏≤‡∏¢
    if valid_count < raw_entities_count:
        print(f"‚ö†Ô∏è Sample {i} Lost Entities: Has {raw_entities_count}, Kept {valid_count}")
        print(f"   Text: {item['text'][:100]}...")

print("\n" + "="*40)
print(f"üìä SUMMARY REPORT")
print(f"Total Entities in JSON: {total_entities_json}")
print(f"Total Entities Validated: {total_entities_valid}")
loss_rate = (total_entities_json - total_entities_valid) / total_entities_json * 100
print(f"üìâ Data Loss Rate: {loss_rate:.2f}%")
print("="*40)

if loss_rate > 10:
    print("‚ùå Critical Issue: Tokenizer cannot align with your entity indices.")
    print("   Solution: Add spaces around entities in your templates.")
else:
    print("‚úÖ Data looks good. The issue might be Learning Rate.")