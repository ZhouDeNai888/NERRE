# NERRE: Named Entity & Relation Extraction for Graph RAG

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0%2B-orange" alt="PyTorch">
  <img src="https://img.shields.io/badge/Transformers-4.30%2B-green" alt="Transformers">
  <img src="https://img.shields.io/badge/License-MIT-yellow" alt="License">
</p>

**NERRE** is a lightweight, multilingual Named Entity Recognition (NER) and Relation Extraction (RE) pipeline designed for Graph RAG applications. It extracts entities and their relationships from text, perfect for building knowledge graphs.

## âœ¨ Features

- ğŸŒ **Multilingual Support**: English, Chinese (Traditional/Simplified), Japanese, Korean, Thai
- ğŸš€ **Zero External Dependencies**: No need for jieba, MeCab, or other tokenizers - the model learns word boundaries automatically
- ğŸ“ **Long Text Support**: Sliding window mechanism handles documents of any length
- ğŸ¯ **Joint NER+RE**: Extracts both entities AND their relationships in one pass
- âš¡ **High Accuracy**: State-of-the-art F1 scores across languages

## ğŸ“Š Benchmark Results

Comparison against popular NER tools on multilingual test set (English + Chinese + Japanese):

| Model | Precision | Recall | **F1 Score** | Notes |
|-------|-----------|--------|--------------|-------|
| **NERRE (Ours)** | 0.959 | **0.986** | **0.973** | Best overall |
| GLiNER | **0.978** | 0.738 | 0.841 | Lower recall |
| spaCy | 0.971 | 0.557 | 0.708 | English-only |

### Key Findings

- **NERRE achieves the highest F1 score (0.973)** with balanced precision and recall
- **GLiNER** has high precision but misses many entities (lower recall)
- **spaCy** is fast but only supports English, missing all CJK entities

### Speed Benchmark (10,000 characters)

| Model | Time | Entities | Triplets | Speed |
|-------|------|----------|----------|-------|
| spaCy | 0.18s | 500 | âŒ | 55,169 chars/s |
| **NERRE Fast** âš¡ | **0.97s** | **876** | âŒ | **10,310 chars/s** |
| GLiNER | 9.26s | 144 | âŒ | 1,080 chars/s |
| **NERRE Full** | **12.74s** | **877** | âœ… 36 | 785 chars/s |

### Key Takeaways

- **NERRE Fast** is **9.5x faster than GLiNER** while finding **6x more entities**
- **NERRE Full** extracts both entities AND relations (triplets) - no other tool does this!
- **NERRE** supports CJK languages natively, unlike spaCy

## ğŸš€ Quick Start

### Installation

```bash
pip install torch transformers huggingface_hub
```

### Usage

```python
from nerre import NERREPipeline

# Load the model
pipe = NERREPipeline.from_pretrained("path/to/model")

# English
result = pipe("Elon Musk founded SpaceX in 2002.")
print(result["entities"])
# [{'text': 'Elon Musk', 'label': 'person', 'score': 0.98},
#  {'text': 'SpaceX', 'label': 'organisation', 'score': 0.96},
#  {'text': '2002', 'label': 'date', 'score': 0.99}]

print(result["triplets"])
# [{'head': 'Elon Musk', 'relation': 'founder_of', 'tail': 'SpaceX', 'confidence': 0.92}]

# Chinese (auto-detected, no external tokenizer needed!)
result = pipe("å°ç©é›»æ˜¯å…¨çƒæœ€å¤§çš„æ™¶ç‰‡è£½é€ å•†ï¼Œå¼µå¿ è¬€æ–¼1987å¹´åœ¨æ–°ç«¹å‰µç«‹ã€‚")
print(result["entities"])
# [{'text': 'å°ç©é›»', 'label': 'organisation', 'score': 0.95},
#  {'text': 'å¼µå¿ è¬€', 'label': 'person', 'score': 0.94},
#  {'text': '1987å¹´', 'label': 'date', 'score': 0.96},
#  {'text': 'æ–°ç«¹', 'label': 'location', 'score': 0.93}]

# Japanese (also auto-detected!)
result = pipe("ã‚½ãƒ‹ãƒ¼ã¯ç››ç”°æ˜­å¤«ã«ã‚ˆã£ã¦1946å¹´ã«æ±äº¬ã§è¨­ç«‹ã•ã‚Œã¾ã—ãŸã€‚")
```

### Fast Mode (NER Only)

For maximum speed when you only need entity extraction:

```python
# Fast Mode: ~10x faster, NER only (no relation extraction)
result = pipe(text, fast_mode=True, extract_relations=False)
# Result: 10,310 chars/sec - faster than GLiNER!

# Full Mode: NER + Relation Extraction (default)
result = pipe(text)
# Result: 785 chars/sec with triplets
```

### Long Text Support

NERRE automatically handles long texts using a sliding window approach:

```python
# Works with texts of any length (10,000+ characters)
long_article = "..." * 10000  # Very long article
result = pipe(long_article)  # No truncation, processes entire document
```

## ğŸ—ï¸ Architecture

NERRE uses a **span-based** approach built on XLM-RoBERTa:

1. **Character-level Span Generation**: For CJK languages, each character is a potential entity boundary
2. **Joint Entity & Relation Classification**: Single forward pass for both tasks
3. **Smart Pair Filtering**: Only checks entity pairs that could form valid relations
4. **Non-Maximum Suppression (NMS)**: Removes overlapping entity predictions
5. **Sliding Window**: Handles documents longer than 512 tokens

### Model Configuration

- **Base Model**: `xlm-roberta-base` (280M parameters)
- **Max Sequence Length**: 512 tokens (with sliding window for longer texts)
- **Entity Types**: `person`, `organisation`, `location`, `date`, `product`, `programlang`
- **Relation Types**: `founder_of`, `ceo_of`, `developed`, `creator_of`, `founded_in`, `released_in`, `located_in`

## ğŸ“ Project Structure

```
NERRE/
â”œâ”€â”€ huggingface_release/    # Production-ready inference code
â”‚   â”œâ”€â”€ nerre.py           # Main pipeline
â”‚   â”œâ”€â”€ model.py           # Model architecture
â”‚   â””â”€â”€ pytorch_model.bin  # Model weights
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train.py           # Training script
â”‚   â””â”€â”€ train_config.py    # Training configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ hf_dataloader.py   # Data loading utilities
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ *.json             # Training data
â”œâ”€â”€ eval/
â”‚   â””â”€â”€ eval.py            # Evaluation script
â”œâ”€â”€ benchmark/
â”‚   â””â”€â”€ benchmark_ner.py   # Benchmark vs GLiNER, spaCy
â””â”€â”€ README.md
```

## ğŸ”§ Training Your Own Model

### 1. Prepare Training Data

Create a JSON file with the following format:

```json
[
  {
    "text": "Elon Musk founded SpaceX in 2002.",
    "entities": [
      {"text": "Elon Musk", "label": "person", "start": 0, "end": 9},
      {"text": "SpaceX", "label": "organisation", "start": 18, "end": 24},
      {"text": "2002", "label": "date", "start": 28, "end": 32}
    ],
    "relations": [
      {"head": "Elon Musk", "tail": "SpaceX", "type": "founder_of"}
    ]
  }
]
```

### 2. Train

```bash
cd train
python train.py
```

### 3. Evaluate

```bash
cd eval
python eval.py
```

## ğŸ“œ License

MIT License

## ğŸ™ Acknowledgments

- [XLM-RoBERTa](https://huggingface.co/xlm-roberta-base) by Facebook AI
- [GLiNER](https://github.com/urchade/GLiNER) for benchmark comparison
- [spaCy](https://spacy.io/) for benchmark comparison

## ğŸ“š Citation

If you use NERRE in your research, please cite:

```bibtex
@software{nerre2024,
  title = {NERRE: Named Entity & Relation Extraction for Graph RAG},
  year = {2024},
  url = {https://github.com/YOUR_USERNAME/NERRE}
}
```